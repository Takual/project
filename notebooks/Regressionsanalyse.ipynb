{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressionanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hier werden die Pickle-Objekte mit den Train-, Val- und Testdaten geladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Zelle 1: Setup und Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Plotstil festlegen (verwenden einen basic matplotlib style)\n",
    "plt.style.use('default')\n",
    "# Seaborn Plotting Style setzen\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'  # Alternativ: 'Liberation Sans', 'Arial Unicode MS'\n",
    "\n",
    "# Display-Optionen\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Zelle 2: Laden der vorverarbeiteten Daten\n",
    "# Laden der Pickle-Datei\n",
    "with open('../data/processed/processed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Extrahieren der einzelnen Komponenten\n",
    "X_train = data['X_train']\n",
    "X_val = data['X_val']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']\n",
    "#scaler = data['scaler']\n",
    "feature_columns = data['feature_columns']\n",
    "target_columns = data['target_columns']\n",
    "\n",
    "# Überprüfung der geladenen Daten\n",
    "print(\"Daten erfolgreich geladen!\")\n",
    "print(f\"\\nTrainings-Set Dimensionen:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"\\nFeature-Spalten:\")\n",
    "print(feature_columns)\n",
    "print(f\"\\nZiel-Spalten:\")\n",
    "print(target_columns)\n",
    "\n",
    "# Zelle 3: Kurze Dateninspektion\n",
    "# Überblick über die Feature-Verteilungen\n",
    "print(\"\\nStatistische Kennzahlen der Features im Trainingsset:\")\n",
    "print(X_train.describe())\n",
    "\n",
    "# Überblick über die Zielvariblen\n",
    "print(\"\\nStatistische Kennzahlen der Zielvariablen im Trainingsset:\")\n",
    "print(y_train.describe())\n",
    "\n",
    "# Visualisierung der Verteilung der Zielvariablen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Verteilung der Zielvariablen')\n",
    "\n",
    "for i, col in enumerate(target_columns):\n",
    "    sns.histplot(data=y_train, x=col, ax=axes[i])\n",
    "    axes[i].set_title(col)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allgemeine Analyse\n",
    "nur erst einmal die Funktionsdefinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features_for_target(X_train, y_train, target_name, feature_columns):\n",
    "    \"\"\"\n",
    "    Führt eine umfassende Feature-Analyse für eine Zielvariable durch\n",
    "    \"\"\"\n",
    "    from sklearn.feature_selection import SelectKBest, f_regression\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Nur numerische Spalten auswählen\n",
    "    numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "    X_numeric = X_train[numeric_features]\n",
    "    \n",
    "    # Imputation der fehlenden Werte\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_numeric_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(X_numeric),\n",
    "        columns=X_numeric.columns\n",
    "    )\n",
    "    \n",
    "    target = y_train[target_name]\n",
    "    results = {}\n",
    "    \n",
    "    # 1. F-Regression\n",
    "    selector = SelectKBest(score_func=f_regression, k='all')\n",
    "    selector.fit(X_numeric_imputed, target)\n",
    "    \n",
    "    f_scores = pd.DataFrame({\n",
    "        'Feature': numeric_features,\n",
    "        'F_Score': selector.scores_,\n",
    "        'P_Value': selector.pvalues_\n",
    "    }).sort_values('F_Score', ascending=False)\n",
    "    \n",
    "    results['f_scores'] = f_scores\n",
    "    \n",
    "    # 2. Lasso\n",
    "    lasso = Lasso(alpha=0.01)\n",
    "    lasso.fit(X_numeric_imputed, target)\n",
    "    \n",
    "    lasso_importance = pd.DataFrame({\n",
    "        'Feature': numeric_features,\n",
    "        'Coefficient': np.abs(lasso.coef_)\n",
    "    }).sort_values('Coefficient', ascending=False)\n",
    "    \n",
    "    results['lasso'] = lasso_importance\n",
    "    \n",
    "    # 3. Korrelationsanalyse\n",
    "    correlation_matrix = X_numeric_imputed.corr()\n",
    "    high_correlations = np.where(np.abs(correlation_matrix) > 0.7)\n",
    "    high_corr_pairs = [(correlation_matrix.index[x], correlation_matrix.columns[y], \n",
    "                        correlation_matrix.iloc[x, y])\n",
    "                       for x, y in zip(*high_correlations) \n",
    "                       if x != y and x < y]\n",
    "    \n",
    "    results['high_correlations'] = high_corr_pairs\n",
    "    \n",
    "    # Visualisierungen\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=f_scores.head(10), x='F_Score', y='Feature')\n",
    "    plt.title(f'Top 10 Features nach F-Score für {target_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Zusätzliche Information über fehlende Werte\n",
    "    missing_values = X_numeric.isnull().sum()\n",
    "    results['missing_values'] = missing_values[missing_values > 0]\n",
    "    \n",
    "    return results\n",
    "# Verwendung:\n",
    "# Führen Sie diese Analyse für jede Zielvariable durch:\n",
    "# results = analyze_features_for_target(X_train, y_train, 'durchschnittliche_wiedergabedauer', feature_columns)\n",
    "# \n",
    "# print(\"Top 10 Features nach F-Score:\")\n",
    "# print(results['f_scores'].head(10))\n",
    "# \n",
    "# print(\"\\nTop 10 Features nach Lasso-Koeffizienten:\")\n",
    "# print(results['lasso'].head(10))\n",
    "# \n",
    "# print(\"\\nStark korrelierte Features:\")\n",
    "# for pair in results['high_correlations']:\n",
    "#     print(f\"{pair[0]} -- {pair[1]}: {pair[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jetzt folgen F-Test-Analyse und Korrelationen zur Zielvariable Wiedergabedauer\n",
    "für die Analyse der verschiedenen Zielvariablen, die entsprechende Zeile auskommentieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse für durchschnittliche Wiedergabedauer\n",
    "#results = analyze_features_for_target(X_train, y_train, 'durchschnittliche_wiedergabedauer', feature_columns)\n",
    "#results = analyze_features_for_target(X_train, y_train, 'aufrufe', feature_columns)\n",
    "results = analyze_features_for_target(X_train, y_train, 'Klickrate der Impressionen (%)', feature_columns)\n",
    "\n",
    "# Ausgabe der Ergebnisse\n",
    "print(\"Top 10 Features nach F-Score:\")\n",
    "print(results['f_scores'].head(10))\n",
    "\n",
    "print(\"\\nTop 10 Features nach Lasso-Koeffizienten:\")\n",
    "print(results['lasso'].head(10))\n",
    "\n",
    "print(\"\\nStark korrelierte Features:\")\n",
    "for pair in results['high_correlations']:\n",
    "    print(f\"{pair[0]} -- {pair[1]}: {pair[2]:.3f}\")\n",
    "\n",
    "print(\"\\nFehlende Werte in den Features:\")\n",
    "print(results['missing_values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hier eine Analyse, inwieweit die Bewertung des Titels eine Rolle spielt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variablen definieren\n",
    "predictor = 'Gestaltung_Thumbnail'  # Hier den Namen der Prädiktor-Variable eintragen\n",
    "\n",
    "target = 'durchschnittliche_wiedergabedauer'                  \n",
    "#target = 'aufrufe'  \n",
    "#target = 'Klickrate der Impressionen (%)'  \n",
    "\n",
    "# Statistiken zur Prädiktor-Variable\n",
    "print(f\"Statistiken zu {predictor}:\")\n",
    "print(X_train[predictor].describe())\n",
    "\n",
    "# Korrelation mit der Zielvariable\n",
    "correlation = X_train[predictor].corr(y_train[target])\n",
    "print(f\"\\nKorrelation zwischen {predictor} und {target}:\", correlation)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train[predictor], y_train[target], alpha=0.5)\n",
    "plt.xlabel(predictor)\n",
    "plt.ylabel(target)\n",
    "plt.title(f'Zusammenhang zwischen {predictor} und {target}')\n",
    "plt.show()\n",
    "\n",
    "# Prüfen auf fehlende Werte\n",
    "missing = X_train[predictor].isnull().sum()\n",
    "print(f\"\\nAnzahl fehlender Werte in {predictor}:\", missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse diverse Boxplots und Streudiagramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zielvariable definieren\n",
    "target_variable = 'Klickrate der Impressionen (%)'\n",
    "\n",
    "# Merkmale definieren\n",
    "categorical_features = [col for col in X_train.columns if X_train[col].dtype == 'bool' or X_train[col].dtype == 'object']\n",
    "numerical_features = [col for col in X_train.columns if X_train[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "# Zusammenführen der Zielvariable und der Prädiktoren\n",
    "train_data = X_train.copy()\n",
    "train_data[target_variable] = y_train[target_variable]\n",
    "\n",
    "# Boxplots für kategoriale Merkmale\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=train_data[feature], y=train_data[target_variable])\n",
    "    plt.title(f'Boxplot: {target_variable} nach {feature}')\n",
    "    plt.ylabel('Klickrate der Impressionen (%)')\n",
    "    plt.xlabel(feature)\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()\n",
    "\n",
    "# Streudiagramme für numerische Merkmale\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=train_data[feature], y=train_data[target_variable])\n",
    "    plt.title(f'Streudiagramm: {target_variable} vs. {feature}')\n",
    "    plt.ylabel('Klickrate der Impressionen (%)')\n",
    "    plt.xlabel(feature)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Umfassende Analyse mit Ausgabe als HTML-Datei (Wegen der Menge an Plots und Daten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "def generate_eda_report(X_train, y_train, target_variable):\n",
    "    html_content = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Explorative Datenanalyse Report</title>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; margin: 20px; max-width: 1200px; margin: 0 auto; }\n",
    "            h1 { color: #2c3e50; }\n",
    "            h2 { color: #34495e; margin-top: 30px; }\n",
    "            .section { margin: 20px 0; }\n",
    "            table { border-collapse: collapse; width: 100%; margin: 15px 0; }\n",
    "            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n",
    "            th { background-color: #f5f5f5; }\n",
    "            .plot-container { margin: 20px 0; text-align: center; }\n",
    "            .explanation { background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 10px 0; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Explorative Datenanalyse Report</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    # Daten zusammenführen\n",
    "    train_data = X_train.copy()\n",
    "    train_data[target_variable] = y_train[target_variable]\n",
    "\n",
    "    # 1. Deskriptive Statistiken nur für numerische Variablen\n",
    "    html_content += \"<h2>1. Deskriptive Statistiken</h2>\"\n",
    "    numerical_features = [col for col in X_train.columns if X_train[col].dtype in ['float64', 'int64']]\n",
    "    categorical_features = [col for col in X_train.columns if col not in numerical_features]\n",
    "    \n",
    "    stats_df = train_data[numerical_features + [target_variable]].describe()\n",
    "    stats_df = stats_df.round(2)\n",
    "    html_content += \"<h3>Numerische Variablen</h3>\"\n",
    "    html_content += stats_df.to_html(classes='dataframe')\n",
    "\n",
    "    # 2. Streudiagramme für numerische Features\n",
    "    html_content += \"<h2>2. Streudiagramme</h2>\"\n",
    "    \n",
    "    for feature in numerical_features:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.scatterplot(data=train_data, x=feature, y=target_variable)\n",
    "        plt.title(f'Streudiagramm: {target_variable} vs. {feature}', pad=20)\n",
    "        plt.ylabel(target_variable)\n",
    "        plt.xlabel(feature)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png', bbox_inches='tight', dpi=150)\n",
    "        buffer.seek(0)\n",
    "        image_png = buffer.getvalue()\n",
    "        buffer.close()\n",
    "        graphic = base64.b64encode(image_png).decode()\n",
    "        \n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"plot-container\">\n",
    "            <img src=\"data:image/png;base64,{graphic}\" width=\"1000\">\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        plt.close()\n",
    "\n",
    "    # 3. Boxplots für kategoriale Features\n",
    "    html_content += \"<h2>3. Boxplots</h2>\"\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.boxplot(data=train_data, x=feature, y=target_variable)\n",
    "        plt.title(f'Boxplot: {target_variable} nach {feature}', pad=20)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png', bbox_inches='tight', dpi=150)\n",
    "        buffer.seek(0)\n",
    "        image_png = buffer.getvalue()\n",
    "        buffer.close()\n",
    "        graphic = base64.b64encode(image_png).decode()\n",
    "        \n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"plot-container\">\n",
    "            <img src=\"data:image/png;base64,{graphic}\" width=\"1000\">\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        plt.close()\n",
    "\n",
    "    # 4. Korrelationsmatrix\n",
    "    html_content += \"<h2>4. Korrelationsmatrix</h2>\"\n",
    "    \n",
    "    # Verbesserte Korrelationsmatrix\n",
    "    corr_matrix = train_data[numerical_features + [target_variable]].corr()\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    mask = np.triu(np.ones_like(corr_matrix), k=1)  # Obere Dreiecksmatrix maskieren\n",
    "    sns.heatmap(corr_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                fmt='.2f',\n",
    "                square=True,\n",
    "                annot_kws={'size': 10},\n",
    "                cbar_kws={'label': 'Korrelationskoeffizient'})\n",
    "    plt.title('Korrelationsmatrix', pad=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format='png', bbox_inches='tight', dpi=150)\n",
    "    buffer.seek(0)\n",
    "    image_png = buffer.getvalue()\n",
    "    buffer.close()\n",
    "    graphic = base64.b64encode(image_png).decode()\n",
    "    \n",
    "    html_content += f\"\"\"\n",
    "    <div class=\"plot-container\">\n",
    "        <img src=\"data:image/png;base64,{graphic}\" width=\"1000\">\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    plt.close()\n",
    "\n",
    "    # Erklärung zur Korrelationsmatrix\n",
    "    html_content += \"\"\"\n",
    "    <div class=\"explanation\">\n",
    "        <h3>Interpretation der Korrelationsmatrix:</h3>\n",
    "        <ul>\n",
    "            <li>Die Werte reichen von -1 bis +1</li>\n",
    "            <li>+1: Perfekt positive Korrelation (wenn A steigt, steigt B proportional)</li>\n",
    "            <li>-1: Perfekt negative Korrelation (wenn A steigt, sinkt B proportional)</li>\n",
    "            <li>0: Keine lineare Korrelation</li>\n",
    "            <li>Farbcodierung: Rot = positive Korrelation, Blau = negative Korrelation</li>\n",
    "            <li>Je intensiver die Farbe, desto stärker die Korrelation</li>\n",
    "        </ul>\n",
    "        <p>Richtwerte für die Interpretation der Korrelationsstärke:</p>\n",
    "        <ul>\n",
    "            <li>0,0 bis 0,2: sehr schwache Korrelation</li>\n",
    "            <li>0,2 bis 0,4: schwache Korrelation</li>\n",
    "            <li>0,4 bis 0,6: moderate Korrelation</li>\n",
    "            <li>0,6 bis 0,8: starke Korrelation</li>\n",
    "            <li>0,8 bis 1,0: sehr starke Korrelation</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    html_content += \"</body></html>\"\n",
    "    \n",
    "    # HTML-Datei speichern\n",
    "    with open('../reports/eda_report.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    return HTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = generate_eda_report(X_train, y_train, target_variable='durchschnittliche_wiedergabedauer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vereinfachte Analyse für verschiedene Zielvariablen. Hier die Definition der Funktion, danach die einzelnen Aufrufe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def flexible_analysis(X_data, y_data, target_variable, predictor_variables, output_dir='analysis_output'):\n",
    "    \"\"\"\n",
    "    Führt eine flexible Analyse mit wählbaren Ziel- und Prädiktorvariablen durch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_data : pandas DataFrame\n",
    "        Prädiktor-Daten (bereits geladen)\n",
    "    y_data : pandas DataFrame\n",
    "        Zielvariablen-Daten (bereits geladen)\n",
    "    target_variable : str\n",
    "        Name der Zielvariable\n",
    "    predictor_variables : list\n",
    "        Liste der Prädiktorvariablen\n",
    "    output_dir : str\n",
    "        Verzeichnis für die Ausgabedateien\n",
    "    \"\"\"\n",
    "    \n",
    "    # Erstelle Ausgabeverzeichnis falls nicht vorhanden\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Kombiniere Daten für die Analyse\n",
    "    analysis_data = pd.DataFrame()\n",
    "    analysis_data[target_variable] = y_data[target_variable]\n",
    "    for pred in predictor_variables:\n",
    "        analysis_data[pred] = X_data[pred]\n",
    "    \n",
    "    # Grundlegende statistische Analyse\n",
    "    print(f\"\\nAnalyse für Zielvariable: {target_variable}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nDeskriptive Statistik für Zielvariable:\")\n",
    "    print(analysis_data[target_variable].describe())\n",
    "    \n",
    "    print(\"\\nDatentypen in der Analyse:\")\n",
    "    print(analysis_data.dtypes)\n",
    "    \n",
    "    # Analyse je Prädiktor\n",
    "    for pred in predictor_variables:\n",
    "        print(f\"\\nAnalyse für Prädiktor: {pred}\")\n",
    "        print(\"-\"*30)\n",
    "        \n",
    "        print(\"\\nDeskriptive Statistik:\")\n",
    "        print(analysis_data[pred].describe())\n",
    "        \n",
    "        # Korrelation nur für numerische Variablen\n",
    "        if pd.api.types.is_numeric_dtype(analysis_data[pred]):\n",
    "            correlation = analysis_data[target_variable].corr(analysis_data[pred])\n",
    "            print(f\"\\nKorrelation mit Zielvariable: {correlation:.4f}\")\n",
    "        \n",
    "        # Für kategorische Variablen (weniger als 10 unique Werte)\n",
    "        if analysis_data[pred].nunique() < 10:\n",
    "            print(\"\\nGruppierte Statistiken:\")\n",
    "            grouped_stats = analysis_data.groupby(pred)[target_variable].agg(['mean', 'std', 'count'])\n",
    "            print(grouped_stats)\n",
    "            \n",
    "            # ANOVA\n",
    "            groups = [group for _, group in analysis_data.groupby(pred)[target_variable]]\n",
    "            if len(groups) >= 2:\n",
    "                f_stat, p_val = stats.f_oneway(*groups)\n",
    "                print(f\"\\nANOVA Test:\")\n",
    "                print(f\"F-Statistik: {f_stat:.4f}\")\n",
    "                print(f\"P-Wert: {p_val:.4f}\")\n",
    "    \n",
    "    # Visualisierungen\n",
    "    fig, axes = plt.subplots(len(predictor_variables), 2, figsize=(12, 6*len(predictor_variables)))\n",
    "    if len(predictor_variables) == 1:\n",
    "        axes = axes.reshape(1, 2)\n",
    "    \n",
    "    for i, pred in enumerate(predictor_variables):\n",
    "        is_categorical = analysis_data[pred].nunique() < 10\n",
    "        \n",
    "        if is_categorical:\n",
    "            # Boxplot\n",
    "            sns.boxplot(x=pred, y=target_variable, data=analysis_data, ax=axes[i,0])\n",
    "            axes[i,0].set_title(f'Boxplot: {target_variable} nach {pred}')\n",
    "            axes[i,0].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Violinplot\n",
    "            # Breite der \"Violine\" zeigt, wie häufig bestimmte Werte vorkommen. Je breiter die Stelle, desto mehr Datenpunkte gibt es \n",
    "            # in diesem Bereich. Je schmaler die Stelle, desto weniger Datenpunkte. \n",
    "            # Zentrale Statistiken: Der schwarze Punkt in der Mitte zeigt den Median. Der dicke schwarze Strich zeigt den \n",
    "            # Interquartilsbereich (IQR, enthält 50% der Daten) \n",
    "            # Dies ist besonders nützlich im Vergleich zu einfachen Boxplots, da die vollständige Verteilung der Daten sichtbar wird, \n",
    "            # einschließlich mehrerer Gipfel oder asymmetrischer Verteilungen.\n",
    "            sns.violinplot(x=pred, y=target_variable, data=analysis_data, ax=axes[i,1])\n",
    "            axes[i,1].set_title(f'Verteilung: {target_variable} nach {pred}')\n",
    "            axes[i,1].tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            # Streudiagramm\n",
    "            sns.scatterplot(x=pred, y=target_variable, data=analysis_data, ax=axes[i,0])\n",
    "            axes[i,0].set_title(f'Streudiagramm: {target_variable} vs {pred}')\n",
    "            \n",
    "            # Regression\n",
    "            #stellt die \"Best-Fit-Line\" oder Ausgleichsgerade durch die Datenpunkte dar, die nach der Methode der kleinsten Quadrate \n",
    "            #(Ordinary Least Squares, OLS) berechnet wird. Diese Methode: Minimiert die Summe der quadrierten vertikalen Abstände \n",
    "            # zwischen den tatsächlichen Datenpunkten und der Regressionslinie. Sucht die optimale Gerade, die durch die Form y = mx + b \n",
    "            # beschrieben wird, wobei:m die Steigung der Linie ist b der y-Achsenabschnitt ist Der blaue schattierte Bereich um die \n",
    "            # Linie zeigt das Konfidenzintervall der Regression an - also den Bereich, in dem die \"wahre\" Regressionslinie \n",
    "            # mit einer bestimmten statistischen Sicherheit liegt.\n",
    "            sns.regplot(x=pred, y=target_variable, data=analysis_data, ax=axes[i,1])\n",
    "            axes[i,1].set_title(f'Regression: {target_variable} vs {pred}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Für Klickrate\n",
    "flexible_analysis(X_train, y_train, \n",
    "                  target_variable='Klickrate der Impressionen (%)',\n",
    "                  predictor_variables=['Gestaltung_Thumbnail', 'Bewertung_Titel', 'video_length_seconds'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für Wiedergabedauer\n",
    "flexible_analysis(X_train, y_train,\n",
    "                  target_variable='durchschnittliche_wiedergabedauer',\n",
    "                  predictor_variables=['Bewertung_Titel', 'Gestaltung_Thumbnail', 'video_length_seconds'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für Aufrufe\n",
    "flexible_analysis(X_train, y_train,\n",
    "                  target_variable='aufrufe',\n",
    "                  predictor_variables=['Gestaltung_Thumbnail', 'Bewertung_Titel', 'video_length_seconds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA, ab hier werden die Ausreißer rausgenommen  \n",
    "Die Ausreisser werden je nach Zielvariable mit verschiedenen Methoden entfernt, IQR oder Z-Score-Methode, Live-Videos sind schon zuvor in den Daten ausgenommen, daher wurde der Code auskommentiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def remove_outliers(data, method='iqr', threshold=1.5):\n",
    "    \"\"\"\n",
    "    Entfernt Ausreißer aus einer Serie basierend auf verschiedenen Methoden.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas Series\n",
    "        Zu bereinigende Daten\n",
    "    method : str\n",
    "        'iqr' für IQR-Methode\n",
    "        'zscore' für Z-Score Methode\n",
    "        'modified_zscore' für modifizierten Z-Score\n",
    "    threshold : float\n",
    "        Schwellenwert für die jeweilige Methode\n",
    "    \"\"\"\n",
    "    if method == 'iqr':\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        return (data >= lower_bound) & (data <= upper_bound)\n",
    "    \n",
    "    elif method == 'zscore':\n",
    "        z_scores = np.abs(stats.zscore(data))\n",
    "        return z_scores < threshold\n",
    "    \n",
    "    elif method == 'modified_zscore':\n",
    "        median = data.median()\n",
    "        mad = stats.median_abs_deviation(data)\n",
    "        modified_z_scores = 0.6745 * (data - median) / mad\n",
    "        return np.abs(modified_z_scores) < threshold\n",
    "    \n",
    "    return pd.Series([True] * len(data))\n",
    "\n",
    "def flexible_analysis(X_data, y_data, target_variable, predictor_variables, \n",
    "                     outlier_method=None, outlier_threshold=1.5, output_dir='analysis_output'):\n",
    "    \"\"\"\n",
    "    Führt flexible Analyse mit optionaler Ausreißerbehandlung durch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_data, y_data : DataFrames mit Prädiktoren und Zielvariablen\n",
    "    target_variable : str, Name der Zielvariable\n",
    "    predictor_variables : list, Liste der Prädiktorvariablen\n",
    "    outlier_method : str oder None, Methode der Ausreißerbehandlung\n",
    "    outlier_threshold : float, Schwellenwert für Ausreißerbehandlung\n",
    "    output_dir : str, Ausgabeverzeichnis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Erstelle Ausgabeverzeichnis falls nicht vorhanden\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Filtere Live-Videos\n",
    "    #non_live_mask = X_data['theme_Live'] == 0\n",
    "    #X_filtered = X_data[non_live_mask]\n",
    "    #y_filtered = y_data[non_live_mask]\n",
    "    X_filtered = X_data\n",
    "    y_filtered = y_data\n",
    "\n",
    "    # Kombiniere Daten für die Analyse\n",
    "    analysis_data = pd.DataFrame()\n",
    "    analysis_data[target_variable] = y_filtered[target_variable]\n",
    "    for pred in predictor_variables:\n",
    "        analysis_data[pred] = X_filtered[pred]\n",
    "    \n",
    "    # Ausreißerbehandlung wenn gewünscht\n",
    "    if outlier_method:\n",
    "        print(f\"Ausreißerbehandlung mit Methode: {outlier_method}, Schwellenwert: {outlier_threshold}\")\n",
    "        initial_size = len(analysis_data)\n",
    "        mask = remove_outliers(analysis_data[target_variable], method=outlier_method, threshold=outlier_threshold)\n",
    "        analysis_data = analysis_data[mask]\n",
    "        print(f\"Entfernte Datenpunkte: {initial_size - len(analysis_data)} ({((initial_size - len(analysis_data))/initial_size*100):.1f}%)\")\n",
    "    \n",
    "    # Grundlegende statistische Analyse\n",
    "    print(f\"\\nAnalyse für Zielvariable: {target_variable}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nDeskriptive Statistik für Zielvariable:\")\n",
    "    print(analysis_data[target_variable].describe())\n",
    "    \n",
    "    # Analyse je Prädiktor\n",
    "    for pred in predictor_variables:\n",
    "        print(f\"\\nAnalyse für Prädiktor: {pred}\")\n",
    "        print(\"-\"*30)\n",
    "        \n",
    "        print(\"\\nDeskriptive Statistik:\")\n",
    "        print(analysis_data[pred].describe())\n",
    "        \n",
    "        # Für kategorische und metrische Variablen\n",
    "        is_categorical = analysis_data[pred].nunique() < 10\n",
    "        if is_categorical:\n",
    "            print(\"\\nGruppierte Statistiken:\")\n",
    "            grouped_stats = analysis_data.groupby(pred)[target_variable].agg(['mean', 'std', 'count'])\n",
    "            print(grouped_stats)\n",
    "            \n",
    "            # ANOVA\n",
    "            groups = [group for _, group in analysis_data.groupby(pred)[target_variable]]\n",
    "            if len(groups) >= 2:\n",
    "                f_stat, p_val = stats.f_oneway(*groups)\n",
    "                print(f\"\\nANOVA Test:\")\n",
    "                print(f\"F-Statistik: {f_stat:.4f}\")\n",
    "                print(f\"P-Wert: {p_val:.4f}\")\n",
    "                \n",
    "                # Effektstärken\n",
    "                categories = sorted(analysis_data[pred].unique())\n",
    "                print(\"\\nEffektstärken (Cohen's d):\")\n",
    "                for i in range(len(categories)):\n",
    "                    for j in range(i + 1, len(categories)):\n",
    "                        cat1 = categories[i]\n",
    "                        cat2 = categories[j]\n",
    "                        group1 = analysis_data[analysis_data[pred] == cat1][target_variable]\n",
    "                        group2 = analysis_data[analysis_data[pred] == cat2][target_variable]\n",
    "                        d = (group1.mean() - group2.mean()) / np.sqrt(\n",
    "                            ((group1.std()**2 + group2.std()**2) / 2))\n",
    "                        print(f\"Kategorie {cat1} vs {cat2}: {d:.4f}\")\n",
    "        else:\n",
    "            correlation = analysis_data[target_variable].corr(analysis_data[pred])\n",
    "            print(f\"\\nKorrelation mit Zielvariable: {correlation:.4f}\")\n",
    "            \n",
    "            # ANOVA (für metrische Prädiktoren)\n",
    "            groups = [group for _, group in analysis_data.groupby(pred)[target_variable]]\n",
    "            if len(groups) >= 2:\n",
    "                f_stat, p_val = stats.f_oneway(*groups)\n",
    "                print(f\"\\nANOVA Test:\")\n",
    "                print(f\"F-Statistik: {f_stat:.4f}\")\n",
    "                print(f\"P-Wert: {p_val:.4f}\")\n",
    "    \n",
    "    # Visualisierungen\n",
    "    fig, axes = plt.subplots(len(predictor_variables), 2, figsize=(12, 6*len(predictor_variables)))\n",
    "    if len(predictor_variables) == 1:\n",
    "        axes = axes.reshape(1, 2)\n",
    "    \n",
    "    for i, pred in enumerate(predictor_variables):\n",
    "        is_categorical = analysis_data[pred].nunique() < 10\n",
    "        \n",
    "        if is_categorical:\n",
    "            # Boxplot\n",
    "            sns.boxplot(x=pred, y=target_variable, data=analysis_data, ax=axes[i,0])\n",
    "            axes[i,0].set_title(f'Boxplot: {target_variable} nach {pred}')\n",
    "            axes[i,0].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Violinplot\n",
    "            sns.violinplot(x=pred, y=target_variable, data=analysis_data, ax=axes[i,1])\n",
    "            axes[i,1].set_title(f'Verteilung: {target_variable} nach {pred}')\n",
    "            axes[i,1].tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            # Streudiagramm\n",
    "            sns.scatterplot(x=pred, y=target_variable, data=analysis_data, ax=axes[i,0])\n",
    "            axes[i,0].set_title(f'Streudiagramm: {target_variable} vs {pred}')\n",
    "            \n",
    "            # Regression\n",
    "            sns.regplot(x=pred, y=target_variable, data=analysis_data, ax=axes[i,1])\n",
    "            axes[i,1].set_title(f'Regression: {target_variable} vs {pred}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return analysis_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielaufrufe:\n",
    "# Für Klickrate mit IQR-Methode\n",
    "flexible_analysis(X_train, y_train,\n",
    "                  target_variable='Klickrate der Impressionen (%)',\n",
    "                  predictor_variables=['Gestaltung_Thumbnail', 'Bewertung_Titel', 'video_length_seconds'],\n",
    "                  outlier_method='iqr',\n",
    "                  outlier_threshold=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für Wiedergabedauer mit modifiziertem Z-Score\n",
    "flexible_analysis(X_train, y_train,\n",
    "                  target_variable='durchschnittliche_wiedergabedauer',\n",
    "                  predictor_variables=['Gestaltung_Thumbnail', 'Bewertung_Titel', 'video_length_seconds'],\n",
    "                  outlier_method='modified_zscore',\n",
    "                  #outlier_method='iqr',                  \n",
    "                  outlier_threshold=3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse für Views\n",
    "# Für Aufrufe mit Z-Score\n",
    "flexible_analysis(X_train, y_train,\n",
    "                  target_variable='aufrufe',\n",
    "                  predictor_variables=['Gestaltung_Thumbnail', 'Bewertung_Titel', 'video_length_seconds'],\n",
    "                  outlier_method='zscore',\n",
    "                  outlier_threshold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressionsanalyse Wiedergabedauer (einfach ohne Parameterausgabe)\n",
    "Die Videolänge hat den stärksten Einfluss auf die Wiedergabedauer, was auch logisch scheint. Die Thumbnail-Gestaltung zeigt ebenfalls einen deutlichen Effekt Die Titelbewertung hat einen signifikanten, aber weniger ausgeprägten Einfluss\n",
    "Alle drei Prädiktoren sind statistisch signifikant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "def load_pickle_data():\n",
    "    \"\"\"Lädt die vorverarbeiteten Daten aus dem Pickle-Objekt\"\"\"\n",
    "    with open('../data/processed/processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def prepare_features(X):\n",
    "    \"\"\"\n",
    "    Bereitet die Features für die Modellierung vor\n",
    "    - Behält nur numerische Spalten\n",
    "    - Entfernt Spalten mit fehlenden Werten\n",
    "    \"\"\"\n",
    "    # Nur numerische Spalten behalten\n",
    "    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    X_numeric = X[numeric_cols]\n",
    "    \n",
    "    # Spalten mit fehlenden Werten entfernen\n",
    "    X_clean = X_numeric.dropna(axis=1)\n",
    "    \n",
    "    print(\"Verwendete Features:\")\n",
    "    print(X_clean.columns.tolist())\n",
    "    \n",
    "    return X_clean\n",
    "\n",
    "def remove_outliers(X, y, method='iqr', threshold=1.5):\n",
    "    \"\"\"Entfernt Ausreißer basierend auf der gewählten Methode - für die Zielvariable\"\"\"\n",
    "    if method == 'iqr':\n",
    "        Q1 = y.quantile(0.25)\n",
    "        Q3 = y.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        mask = ~((y < (Q1 - threshold * IQR)) | (y > (Q3 + threshold * IQR)))\n",
    "    elif method == 'zscore':\n",
    "        z_scores = np.abs(stats.zscore(y))\n",
    "        mask = z_scores < threshold\n",
    "    \n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "    \n",
    "    print(f\"Ausreißerbehandlung mit Methode: {method}, Schwellenwert: {threshold}\")\n",
    "    print(f\"Entfernte Datenpunkte: {len(y) - len(y_clean)} ({(1 - len(y_clean)/len(y))*100:.1f}%)\")\n",
    "    \n",
    "    return X_clean, y_clean\n",
    "\n",
    "def train_models(X, y):\n",
    "    \"\"\"Trainiert verschiedene Regressionsmodelle und vergleicht ihre Performance\"\"\"\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Lasso Regression': Lasso(alpha=1.0)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Cross-Validation Scores\n",
    "        cv_scores = cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "        rmse_scores = np.sqrt(-cv_scores)\n",
    "        \n",
    "        # Modell auf gesamten Datensatz trainieren\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'cv_rmse_mean': rmse_scores.mean(),\n",
    "            'cv_rmse_std': rmse_scores.std(),\n",
    "            'r2_score': r2_score(y, y_pred),\n",
    "            'mae': mean_absolute_error(y, y_pred),\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        # Feature Importance für Linear und Ridge Regression\n",
    "        if name in ['Linear Regression', 'Ridge Regression']:\n",
    "            results[name]['feature_importance'] = pd.DataFrame({\n",
    "                'feature': X.columns,\n",
    "                'importance': model.coef_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_residuals(y_true, y_pred, title=\"Residuenanalyse\"):\n",
    "    \"\"\"Führt eine detaillierte Residuenanalyse durch\"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    # Plot erstellen\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    # Residuen vs. vorhergesagte Werte\n",
    "    axes[0,0].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axes[0,0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0,0].set_xlabel('Vorhergesagte Werte')\n",
    "    axes[0,0].set_ylabel('Residuen')\n",
    "    axes[0,0].set_title('Residuen vs. Vorhergesage')\n",
    "    \n",
    "    # Q-Q Plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot')\n",
    "    \n",
    "    # Histogramm der Residuen\n",
    "    axes[1,0].hist(residuals, bins=30)\n",
    "    axes[1,0].set_xlabel('Residuen')\n",
    "    axes[1,0].set_ylabel('Häufigkeit')\n",
    "    axes[1,0].set_title('Verteilung der Residuen')\n",
    "    \n",
    "    # Tatsächliche vs. vorhergesagte Werte\n",
    "    axes[1,1].scatter(y_true, y_pred, alpha=0.5)\n",
    "    axes[1,1].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    axes[1,1].set_xlabel('Tatsächliche Werte')\n",
    "    axes[1,1].set_ylabel('Vorhergesagte Werte')\n",
    "    axes[1,1].set_title('Vorhersage vs. Realität')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiken der Residuen\n",
    "    print(\"\\nStatistiken der Residuen:\")\n",
    "    print(f\"Mittelwert: {residuals.mean():.4f}\")\n",
    "    print(f\"Standardabweichung: {residuals.std():.4f}\")\n",
    "    print(f\"Schiefe: {stats.skew(residuals):.4f}\")\n",
    "    print(f\"Kurtosis: {stats.kurtosis(residuals):.4f}\")\n",
    "    \n",
    "    # Shapiro-Wilk Test auf Normalverteilung\n",
    "    _, p_value = stats.shapiro(residuals)\n",
    "    print(f\"\\nShapiro-Wilk Test p-Wert: {p_value:.4f}\")\n",
    "    \n",
    "    # Breusch-Pagan Test auf Homoskedastizität\n",
    "    _, p_value = stats.levene(residuals, y_pred)\n",
    "    print(f\"Breusch-Pagan Test p-Wert: {p_value:.4f}\")\n",
    "\n",
    "def main():\n",
    "    # Daten laden\n",
    "    data = load_pickle_data()\n",
    "    \n",
    "    # Features vorbereiten\n",
    "    X_train = prepare_features(data['X_train'])\n",
    "    X_val = prepare_features(data['X_val'])\n",
    "    \n",
    "    # Zielvariablen extrahieren\n",
    "    y_train = data['y_train']['durchschnittliche_wiedergabedauer']\n",
    "    y_val = data['y_val']['durchschnittliche_wiedergabedauer']\n",
    "    \n",
    "    # Ausreißer entfernen\n",
    "    X_train_clean, y_train_clean = remove_outliers(X_train, y_train, method='iqr', threshold=1.5)\n",
    "    \n",
    "    # Modelle trainieren\n",
    "    results = train_models(X_train_clean, y_train_clean)\n",
    "    \n",
    "    # Ergebnisse ausgeben\n",
    "    print(\"\\nModellvergleich:\")\n",
    "    for name, result in results.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"Cross-validated RMSE: {result['cv_rmse_mean']:.2f} (+/- {result['cv_rmse_std']:.2f})\")\n",
    "        print(f\"R² Score: {result['r2_score']:.4f}\")\n",
    "        print(f\"MAE: {result['mae']:.4f}\")\n",
    "        \n",
    "        # Feature Importance ausgeben falls vorhanden\n",
    "        if 'feature_importance' in result:\n",
    "            print(\"\\nTop 5 wichtigste Features:\")\n",
    "            print(result['feature_importance'].head())\n",
    "    \n",
    "    # Bestes Modell auswählen\n",
    "    best_model = max(results.items(), key=lambda x: x[1]['r2_score'])\n",
    "    print(f\"\\nBestes Modell: {best_model[0]}\")\n",
    "    \n",
    "    # Validierung auf Validierungsdatensatz\n",
    "    final_predictions = best_model[1]['model'].predict(X_val)\n",
    "    val_r2 = r2_score(y_val, final_predictions)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, final_predictions))\n",
    "    print(f\"\\nValidierungsergebnisse:\")\n",
    "    print(f\"R² Score: {val_r2:.4f}\")\n",
    "    print(f\"RMSE: {val_rmse:.2f}\")\n",
    "    \n",
    "    # Residuenanalyse für bestes Modell\n",
    "    analyze_residuals(y_train_clean, best_model[1]['predictions'], \n",
    "                     title=f\"Residuenanalyse - {best_model[0]}\")\n",
    "    \n",
    "    return results, best_model[1]['model']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, best_model = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressionsanalyse mit Modell mit spez. Vorgaben für Features (TOP-ERGEBNIS) Modellierung durchschn. Wiedergabedauer\n",
    "mit AUsgabe der Modellparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "def load_and_filter_data():\n",
    "    \"\"\"Lädt die Pickle-Daten und filtert die relevanten Features\"\"\"\n",
    "    with open('../data/processed/processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # Zeitliche Features aus publish_date extrahieren\n",
    "    for dataset in ['X_train', 'X_val', 'X_test']:\n",
    "        if 'publish_date' in data[dataset].columns:\n",
    "            # Konvertiere zu datetime mit deutschem Format\n",
    "            data[dataset]['publish_date'] = pd.to_datetime(data[dataset]['publish_date'], \n",
    "                                                         format='%d.%m.%Y %H:%M')\n",
    "            # Extrahiere Stunde und Wochentag\n",
    "            data[dataset]['publish_hour'] = data[dataset]['publish_date'].dt.hour\n",
    "            data[dataset]['publish_weekday'] = data[dataset]['publish_date'].dt.dayofweek\n",
    "    \n",
    "    # Liste der zu behaltenden Features\n",
    "    keep_features = [\n",
    "        'video_length_seconds',\n",
    "        'Gestaltung_Thumbnail',\n",
    "        'Bewertung_Titel',\n",
    "        'publish_hour',\n",
    "        'publish_weekday'\n",
    "    ]\n",
    "    \n",
    "    # Thema-Dummy-Variablen finden und hinzufügen\n",
    "    #theme_features = [col for col in data['X_train'].columns if col.startswith('theme_')]\n",
    "    # Thema-Dummy-Variablen finden und hinzufügen, aber ohne \"Live\"-Dummy, da ja auch keine Live-Videos mehr dabei sind\n",
    "    theme_features = [col for col in data['X_train'].columns if col.startswith('theme_') and not col.endswith('Live')]\n",
    "    keep_features.extend(theme_features)\n",
    "    \n",
    "    # Features filtern\n",
    "    X_train = data['X_train'][keep_features]\n",
    "    X_val = data['X_val'][keep_features]\n",
    "    X_test = data['X_test'][keep_features]\n",
    "    \n",
    "    # Zielvariable\n",
    "    y_train = data['y_train']['durchschnittliche_wiedergabedauer']\n",
    "    y_val = data['y_val']['durchschnittliche_wiedergabedauer']\n",
    "    y_test = data['y_test']['durchschnittliche_wiedergabedauer']\n",
    "    \n",
    "    print(\"Verwendete Features:\")\n",
    "    print(X_train.columns.tolist())\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def remove_outliers(X, y, method='iqr', threshold=1.5):\n",
    "    \"\"\"Entfernt Ausreißer basierend auf der gewählten Methode\"\"\"\n",
    "    if method == 'iqr':\n",
    "        Q1 = y.quantile(0.25)\n",
    "        Q3 = y.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        mask = ~((y < (Q1 - threshold * IQR)) | (y > (Q3 + threshold * IQR)))\n",
    "    elif method == 'zscore':\n",
    "        z_scores = np.abs(stats.zscore(y))\n",
    "        mask = z_scores < threshold\n",
    "    \n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "    \n",
    "    print(f\"Ausreißerbehandlung mit Methode: {method}, Schwellenwert: {threshold}\")\n",
    "    print(f\"Entfernte Datenpunkte: {len(y) - len(y_clean)} ({(1 - len(y_clean)/len(y))*100:.1f}%)\")\n",
    "    \n",
    "    return X_clean, y_clean\n",
    "\n",
    "def train_models(X, y):\n",
    "    \"\"\"Trainiert verschiedene Regressionsmodelle und vergleicht ihre Performance\"\"\"\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Lasso Regression': Lasso(alpha=1.0)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Cross-Validation Scores\n",
    "        cv_scores = cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "        rmse_scores = np.sqrt(-cv_scores)\n",
    "        \n",
    "        # Modell auf gesamten Datensatz trainieren\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'cv_rmse_mean': rmse_scores.mean(),\n",
    "            'cv_rmse_std': rmse_scores.std(),\n",
    "            'r2_score': r2_score(y, y_pred),\n",
    "            'mae': mean_absolute_error(y, y_pred),\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        # Feature Importance für Linear und Ridge Regression\n",
    "        if name in ['Linear Regression', 'Ridge Regression']:\n",
    "            results[name]['feature_importance'] = pd.DataFrame({\n",
    "                'feature': X.columns,\n",
    "                'importance': model.coef_\n",
    "            }).sort_values('importance', key=lambda x: abs(x), ascending=False)  # Sortierung nach absolutem Wert\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_residuals(y_true, y_pred, title=\"Residuenanalyse\"):\n",
    "    \"\"\"Führt eine detaillierte Residuenanalyse durch\"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    # Plot erstellen\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    # Residuen vs. vorhergesagte Werte\n",
    "    axes[0,0].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axes[0,0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0,0].set_xlabel('Vorhergesagte Werte')\n",
    "    axes[0,0].set_ylabel('Residuen')\n",
    "    axes[0,0].set_title('Residuen vs. Vorhergesage')\n",
    "    \n",
    "    # Q-Q Plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot')\n",
    "    \n",
    "    # Histogramm der Residuen\n",
    "    axes[1,0].hist(residuals, bins=30)\n",
    "    axes[1,0].set_xlabel('Residuen')\n",
    "    axes[1,0].set_ylabel('Häufigkeit')\n",
    "    axes[1,0].set_title('Verteilung der Residuen')\n",
    "    \n",
    "    # Tatsächliche vs. vorhergesagte Werte\n",
    "    axes[1,1].scatter(y_true, y_pred, alpha=0.5)\n",
    "    axes[1,1].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    axes[1,1].set_xlabel('Tatsächliche Werte')\n",
    "    axes[1,1].set_ylabel('Vorhergesagte Werte')\n",
    "    axes[1,1].set_title('Vorhersage vs. Realität')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiken der Residuen\n",
    "    print(\"\\nStatistiken der Residuen:\")\n",
    "    print(f\"Mittelwert: {residuals.mean():.4f}\")\n",
    "    print(f\"Standardabweichung: {residuals.std():.4f}\")\n",
    "    print(f\"Schiefe: {stats.skew(residuals):.4f}\")\n",
    "    print(f\"Kurtosis: {stats.kurtosis(residuals):.4f}\")\n",
    "    \n",
    "    # Shapiro-Wilk Test auf Normalverteilung\n",
    "    _, p_value = stats.shapiro(residuals)\n",
    "    print(f\"\\nShapiro-Wilk Test p-Wert: {p_value:.4f}\")\n",
    "    \n",
    "    # Breusch-Pagan Test auf Homoskedastizität\n",
    "    _, p_value = stats.levene(residuals, y_pred)\n",
    "    print(f\"Breusch-Pagan Test p-Wert: {p_value:.4f}\")\n",
    "\n",
    "def main():\n",
    "    # 1) Daten laden und filtern\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_and_filter_data()\n",
    "    \n",
    "    # 2) Ausreißer entfernen\n",
    "    X_train_clean, y_train_clean = remove_outliers(X_train, y_train, method='iqr', threshold=1.5)\n",
    "    \n",
    "    # 3) Modelle trainieren\n",
    "    results = train_models(X_train_clean, y_train_clean)\n",
    "    \n",
    "    # 4) Ergebnisse ausgeben\n",
    "    print(\"\\nModellvergleich:\")\n",
    "    for name, result in results.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"Cross-validated RMSE: {result['cv_rmse_mean']:.2f} (+/- {result['cv_rmse_std']:.2f})\")\n",
    "        print(f\"R² Score: {result['r2_score']:.4f}\")\n",
    "        print(f\"MAE: {result['mae']:.4f}\")\n",
    "        \n",
    "        if 'feature_importance' in result:\n",
    "            print(\"\\nFeature Importance:\")\n",
    "            print(result['feature_importance'])\n",
    "    \n",
    "    # 5) Bestes Modell auswählen\n",
    "    best_model_tuple = max(results.items(), key=lambda x: x[1]['r2_score'])  # CHANGED: rename to _tuple\n",
    "    best_model_name = best_model_tuple[0]   # z.B. \"Linear Regression\"\n",
    "    best_model_info = best_model_tuple[1]   # dictionary mit {'model':..., 'predictions':..., ...}\n",
    "\n",
    "    sklearn_model = best_model_info['model']  # ADDED: Hol dir das Modellobjekt\n",
    "    print(f\"\\nBestes Modell: {best_model_name}\")\n",
    "    \n",
    "    # 6) Validierung\n",
    "    final_predictions = sklearn_model.predict(X_val)\n",
    "    val_r2 = r2_score(y_val, final_predictions)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, final_predictions))\n",
    "    print(f\"\\nValidierungsergebnisse:\")\n",
    "    print(f\"R² Score: {val_r2:.4f}\")\n",
    "    print(f\"RMSE: {val_rmse:.2f}\")\n",
    "    \n",
    "    # 7) Residuenanalyse\n",
    "    analyze_residuals(y_train_clean, best_model_info['predictions'], \n",
    "                     title=f\"Residuenanalyse - {best_model_name}\")\n",
    "    \n",
    "    # 8) Intercept & Coefficients für lineare Modelle ausgeben - ADDED\n",
    "    if hasattr(sklearn_model, 'coef_') and hasattr(sklearn_model, 'intercept_'):\n",
    "        intercept = sklearn_model.intercept_\n",
    "        coefficients = sklearn_model.coef_\n",
    "        features = X_train_clean.columns\n",
    "        \n",
    "        print(\"\\n--- Lineare Modellformel / Koeffizienten ---\")\n",
    "        print(f\"Intercept: {intercept:.4f}\")\n",
    "        \n",
    "        for feat, coef_val in zip(features, coefficients):\n",
    "            print(f\"{feat}: {coef_val:.4f}\")\n",
    "    \n",
    "    # 9) Rückgabe (unverändert, nur best_model_tuple -> best_model_info['model'])\n",
    "    return results, sklearn_model  # CHANGED: Return the sklearn_model directly\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, best_model = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Feature Selection, (für ausgewählte Prädiktoren)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "nur für ausgewählte Prädiktoren, die man auch beeinflussen kann.\n",
    "\n",
    "Dauer des Videos (z. B. video_length_seconds)\n",
    "Thema (Dummy-Variablen wie theme_Bilder, theme_Krieg, theme_Politik, theme_Sonstiges, theme_Wirtschaft – falls du noch andere Themen hast, kannst du sie hinzufügen)\n",
    "Titel-Bewertung (z. B. Bewertung_Titel)\n",
    "Thumbnail-Gestaltung (z. B. Gestaltung_Thumbnail)\n",
    "Wochentag der Veröffentlichung (publish_weekday)\n",
    "Stunde der Veröffentlichung (publish_hour)\n",
    "\n",
    "Alle anderen Variablen, auf die du keinen direkten Einfluss hast (z. B. bereits vorhandene Klick-Statistiken oder sonstige automatisierte Messgrößen), werden hier nicht berücksichtigt.\n",
    "\n",
    "**Erläuterungen**\n",
    "\n",
    "candidate_features enthält genau die Spalten, auf die du direkt Einfluss nehmen kannst (Videolänge, Thema, Titelbewertung, Thumbnail, Publikationszeit, etc.).\n",
    "Wenn in deinem DataFrame noch weitere Spalten stehen, filterst du sie raus, damit der Forward-Selection-Prozess nur auf diesen beeinflussbaren Variablen arbeitet.\n",
    "theme_Live wird ignoriert/gelöscht, falls es existiert, weil du es explizit ausschließen wolltest.\n",
    "Ablauf:\n",
    "Start: Keine Features im Modell.\n",
    "Schrittweise:\n",
    "Probiere jedes noch nicht ausgewählte Feature einzeln aus.\n",
    "Berechne z. B. den Cross-Validation-R² (Standard: 5-Fold).\n",
    "Nimm das Feature, das den Score am meisten steigert.\n",
    "Wiederhole, bis alle Features eingebaut wurden oder bis du entscheidest, dass dir die Performance reicht.\n",
    "Am Ende findest du eine Reihenfolge der „wichtigsten“ Features und kannst je nach Komplexität ein Modell mit all diesen (oder nur den Top-k) Merkmalen nutzen.\n",
    "**Was du damit gewinnst**\n",
    "\n",
    "Du siehst zum Beispiel, ob Gestaltung_Thumbnail zuerst aufgenommen wird (starker Effekt) oder ob vielleicht theme_Krieg einen größeren Einfluss hat.\n",
    "Du erfährst, ob publish_hour oder publish_weekday überhaupt eine nennenswerte Verbesserung bringen, oder erst ganz am Ende (falls überhaupt).\n",
    "Das Endergebnis kann dir helfen, gezielt an den Stellschrauben zu drehen, die du tatsächlich beeinflussen kannst, anstatt das Modell mit irrelevanten oder unveränderbaren Features zu überfrachten.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from itertools import combinations\n",
    "\n",
    "#TARGET_VAR = \"durchschnittliche_wiedergabedauer\"  # Zielvariable ist durchschn. Wiedergabedauer\n",
    "#TARGET_VAR = \"aufrufe\"  # Aufrufe (views)\n",
    "TARGET_VAR = \"Klickrate der Impressionen (%)\"  # Klickrate der Impressionen\n",
    "\n",
    "def forward_feature_selection(X, y, \n",
    "                              scoring='r2', \n",
    "                              n_splits=5, \n",
    "                              random_state=42):\n",
    "    \"\"\"\n",
    "    Führt eine Vorwärtsselektion (Forward Feature Selection) durch,\n",
    "    basierend auf dem ausgewählten Scoring (Standard: R²).\n",
    "    \n",
    "    :param X: pd.DataFrame mit den rein beeinflussbaren Features\n",
    "    :param y: pd.Series oder np.array mit der Zielvariable\n",
    "    :param scoring: 'r2' oder 'neg_mean_squared_error'\n",
    "    :param n_splits: Anzahl K-Folds in der Cross-Validation\n",
    "    :param random_state: Zufallssamen für Reproduzierbarkeit\n",
    "    :return: \n",
    "       selected_features (List): Features in der Reihenfolge ihrer Aufnahme\n",
    "       best_models (Dict): Zwischenergebnisse \n",
    "           Key = Tuple(Features), Value = (Train_R2, Train_RMSE, Train_MAE, Modellobjekt)\n",
    "    \"\"\"\n",
    "    \n",
    "    all_features = list(X.columns)  # Alle möglichen Features, die DU beeinflussen kannst\n",
    "    \n",
    "    selected_features = []          # Start: Keine Features\n",
    "    remaining_features = set(all_features)\n",
    "    best_models = {}\n",
    "    \n",
    "    while remaining_features:\n",
    "        best_score = -np.inf\n",
    "        best_feature = None\n",
    "        \n",
    "        # Teste jedes Feature, das noch nicht ausgewählt ist\n",
    "        for feature in remaining_features:\n",
    "            current_features = selected_features + [feature]\n",
    "            X_sub = X[current_features]\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            \n",
    "            # Cross-Validation (für R² oder neg_mean_squared_error)\n",
    "            kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "            \n",
    "            if scoring == 'r2':\n",
    "                cv_scores = cross_val_score(model, X_sub, y, cv=kfold, scoring='r2')\n",
    "                mean_score = cv_scores.mean()\n",
    "            elif scoring == 'neg_mean_squared_error':\n",
    "                cv_scores = cross_val_score(model, X_sub, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "                # Hier sind die Werte negativ, da MSE \"minimiert\" wird. \n",
    "                # Man könnte -mean_score nehmen, um \"maximieren\" zu simulieren. \n",
    "                # Für Forward-Selection reicht es oft, den Mittelwert direkt zu vergleichen.\n",
    "                mean_score = cv_scores.mean()\n",
    "            else:\n",
    "                raise ValueError(\"Bitte scoring='r2' oder 'neg_mean_squared_error' verwenden.\")\n",
    "            \n",
    "            # Wähle jenes Feature, das den besten (höchsten) Score liefert\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_feature = feature\n",
    "        \n",
    "        if best_feature is not None:\n",
    "            selected_features.append(best_feature)\n",
    "            remaining_features.remove(best_feature)\n",
    "            \n",
    "            # Trainiere ein finales Modell auf dem gesamten Datensatz (Train) mit den ausgewählten Features\n",
    "            X_sub = X[selected_features]\n",
    "            final_model = LinearRegression()\n",
    "            final_model.fit(X_sub, y)\n",
    "            \n",
    "            # Metriken auf dem gesamten Trainingsset (optional, zur Info)\n",
    "            y_pred = final_model.predict(X_sub)\n",
    "            train_r2 = r2_score(y, y_pred)\n",
    "            train_rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "            train_mae = np.mean(np.abs(y - y_pred))\n",
    "            \n",
    "            best_models[tuple(selected_features)] = (train_r2, train_rmse, train_mae, final_model)\n",
    "            \n",
    "            print(f\"Feature hinzugefügt: {best_feature}. \"\n",
    "                  f\"Aktuelle Liste: {selected_features}. \"\n",
    "                  f\"(Train R²={train_r2:.4f}, RMSE={train_rmse:.2f}, MAE={train_mae:.2f})\")\n",
    "        else:\n",
    "            # Wenn kein Feature den Score steigert, brechen wir\n",
    "            break\n",
    "    \n",
    "    return selected_features, best_models\n",
    "\n",
    "def main():\n",
    "    # 1) Daten laden\n",
    "    with open('../data/processed/processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Zeitliche Features aus publish_date extrahieren\n",
    "    for dataset in ['X_train', 'X_val']:\n",
    "        if 'publish_date' in data[dataset].columns:\n",
    "            data[dataset]['publish_date'] = pd.to_datetime(\n",
    "                data[dataset]['publish_date'], \n",
    "                format='%d.%m.%Y %H:%M'\n",
    "            )\n",
    "            data[dataset]['publish_hour'] = data[dataset]['publish_date'].dt.hour\n",
    "            data[dataset]['publish_weekday'] = data[dataset]['publish_date'].dt.dayofweek\n",
    "    \n",
    "    # 2) Trainings- und Validierungsdaten laden\n",
    "    X_train = data['X_train'].copy()\n",
    "    y_train = data['y_train'][TARGET_VAR].copy()\n",
    "    \n",
    "    X_val = data['X_val'].copy()\n",
    "    y_val = data['y_val'][TARGET_VAR].copy()\n",
    "    \n",
    "    # 3) Nur beeinflussbare Features (ohne theme_Live) behalten\n",
    "    candidate_features = [\n",
    "        'video_length_seconds',\n",
    "        'Gestaltung_Thumbnail',\n",
    "        'Bewertung_Titel',\n",
    "        'publish_hour',\n",
    "        'publish_weekday',\n",
    "        'theme_Bilder',\n",
    "        'theme_Krieg',\n",
    "        'theme_Politik',\n",
    "        'theme_Sonstiges',\n",
    "        'theme_Wirtschaft'\n",
    "    ]\n",
    "    \n",
    "    # Falls \"theme_Live\" existiert, ausschließen\n",
    "    if 'theme_Live' in X_train.columns:\n",
    "        X_train.drop(columns=['theme_Live'], inplace=True, errors='ignore')\n",
    "    if 'theme_Live' in X_val.columns:\n",
    "        X_val.drop(columns=['theme_Live'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Filter auf die Candidate-Features\n",
    "    X_train = X_train[candidate_features]\n",
    "    X_val = X_val[candidate_features]\n",
    "    \n",
    "    # 4) Forward Feature Selection (auf Trainingsdaten)\n",
    "    selected_feats, models_dict = forward_feature_selection(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        scoring='r2',           # oder 'neg_mean_squared_error'\n",
    "        n_splits=5, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- Vorwärtsselektion abgeschlossen ---\")\n",
    "    print(\"Ausgewählte Features (in Reihenfolge der Aufnahme):\")\n",
    "    for feat in selected_feats:\n",
    "        print(\"  \", feat)\n",
    "    \n",
    "    # 5) Bestes Modell anhand des höchsten Train-R²\n",
    "    best_key = max(models_dict.keys(), key=lambda k: models_dict[k][0])  # Sortiere nach R²\n",
    "    best_r2, best_rmse, best_mae, best_model = models_dict[best_key]\n",
    "    \n",
    "    # 6) Test des besten Modells auf dem Validierungsdatensatz\n",
    "    X_val_sub = X_val[list(best_key)]\n",
    "    y_val_pred = best_model.predict(X_val_sub)\n",
    "    \n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    \n",
    "    print(\"\\n--- Validierungsergebnisse mit dem besten Modell ---\")\n",
    "    print(f\"Train R² = {best_r2:.4f}, Val R² = {val_r2:.4f}\")\n",
    "    print(f\"Train RMSE = {best_rmse:.2f}, Val RMSE = {val_rmse:.2f}\")\n",
    "    print(f\"Train MAE = {best_mae:.2f}, Val MAE = {val_mae:.2f}\")\n",
    "    \n",
    "    # 7) Ausgabe der Koeffizienten (optional)\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': list(best_key),\n",
    "        'Coefficient': best_model.coef_\n",
    "    })\n",
    "    print(\"\\nKoeffizienten des finalen Modells:\")\n",
    "    print(\"Intercept:\", best_model.intercept_)\n",
    "    print(coef_df)\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # 8) MODEL & INFO SPEICHERN\n",
    "    # -----------------------------------\n",
    "    # Ordner \"models\" anlegen, falls nicht vorhanden\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "    \n",
    "    # Model-Dateiname (z.B. best_model_durchschnittliche_wiedergabedauer.pkl)\n",
    "    model_filename = f\"../models/best_model_{TARGET_VAR}.pkl\"\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f\"\\nModell gespeichert in: {model_filename}\")\n",
    "    \n",
    "    # Wichtigste Kennzahlen & Parameter als JSON speichern\n",
    "    model_info = {\n",
    "        \"target_variable\": TARGET_VAR,\n",
    "        \"selected_features\": list(best_key),\n",
    "        \"train_r2\": round(best_r2, 4),\n",
    "        \"train_rmse\": round(best_rmse, 2),\n",
    "        \"train_mae\": round(best_mae, 2),\n",
    "        \"val_r2\": round(val_r2, 4),\n",
    "        \"val_rmse\": round(val_rmse, 2),\n",
    "        \"val_mae\": round(val_mae, 2),\n",
    "        \"coefficients\": {\n",
    "            \"Intercept\": round(best_model.intercept_, 4)\n",
    "        }\n",
    "    }\n",
    "    # Coefficients-Detail\n",
    "    for feat, coef_val in zip(best_key, best_model.coef_):\n",
    "        model_info[\"coefficients\"][feat] = round(float(coef_val), 4)\n",
    "    \n",
    "    # JSON-Dateiname (z.B. best_model_durchschnittliche_wiedergabedauer_info.json)\n",
    "    json_filename = f\"../models/best_model_{TARGET_VAR}_info.json\"\n",
    "    with open(json_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(model_info, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nModell-Informationen als JSON gespeichert in: {json_filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ladevorgang Model und Kennzahlen testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "loaded_model = joblib.load('../models/best_model_durchschnittliche_wiedergabedauer.pkl')\n",
    "\n",
    "#Jetzt kann mit loaded_model.predict(X) das Model aufgerufen werden\n",
    "\n",
    "json_filename = \"../models/best_model_durchschnittliche_wiedergabedauer_info.json\"\n",
    "\n",
    "# JSON laden\n",
    "with open(json_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    model_info = json.load(f)\n",
    "\n",
    "# Kennzahlen ausgeben\n",
    "print(\"Zielvariable:\", model_info[\"target_variable\"])\n",
    "print(\"Ausgewählte Features:\", model_info[\"selected_features\"])\n",
    "print(\"Train R²:\", model_info[\"train_r2\"])\n",
    "print(\"Train RMSE:\", model_info[\"train_rmse\"])\n",
    "print(\"Train MAE:\", model_info[\"train_mae\"])\n",
    "print(\"Val R²:\", model_info[\"val_r2\"])\n",
    "print(\"Val RMSE:\", model_info[\"val_rmse\"])\n",
    "print(\"Val MAE:\", model_info[\"val_mae\"])\n",
    "\n",
    "# Falls du die Koeffizienten (Intercept + weitere) ausgeben möchtest:\n",
    "print(\"\\nKoeffizienten:\")\n",
    "for feat, coef_val in model_info[\"coefficients\"].items():\n",
    "    print(f\"{feat}: {coef_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verteilungstest zwischen Validierung und Trainings-Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_p_values(data_train, data_val, title=\"\"):\n",
    "    \"\"\"Berechnet p-Werte für Verteilungsvergleiche\"\"\"\n",
    "    if pd.api.types.is_numeric_dtype(data_train):\n",
    "        try:\n",
    "            # Kolmogorov-Smirnov Test\n",
    "            _, ks_pval = stats.ks_2samp(data_train, data_val)\n",
    "            # Mann-Whitney U Test\n",
    "            _, mw_pval = stats.mannwhitneyu(data_train, data_val, alternative='two-sided')\n",
    "            \n",
    "            print(f\"{title}:\")\n",
    "            print(f\"KS-Test p-Wert: {ks_pval:.4f}\")\n",
    "            print(f\"MW-Test p-Wert: {mw_pval:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "        except Exception as e:\n",
    "            print(f\"{title}: Tests nicht möglich - {str(e)}\")\n",
    "\n",
    "def compare_all_p_values(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"Vergleicht p-Werte für alle Features und Zielvariable\"\"\"\n",
    "    print(\"P-Werte für Features:\")\n",
    "    print(\"=\" * 50)\n",
    "    for column in X_train.columns:\n",
    "        check_p_values(X_train[column], X_val[column], title=column)\n",
    "    \n",
    "    print(\"\\nP-Werte für Zielvariable:\")\n",
    "    print(\"=\" * 50)\n",
    "    check_p_values(\n",
    "        y_train['durchschnittliche_wiedergabedauer'],\n",
    "        y_val['durchschnittliche_wiedergabedauer'],\n",
    "        \"Wiedergabedauer\"\n",
    "    )\n",
    "\n",
    "# Aufruf:\n",
    "compare_all_p_values(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_theme_distribution(X_train, X_val, X_test):\n",
    "    # Für jedes Theme-Feature (theme_*)\n",
    "    theme_cols = [col for col in X_train.columns if col.startswith('theme_')]\n",
    "    \n",
    "    for theme in theme_cols:\n",
    "        print(f\"\\nVerteilung für {theme}:\")\n",
    "        print(f\"Training:   {X_train[theme].mean():.3f}\")\n",
    "        print(f\"Validierung: {X_val[theme].mean():.3f}\")\n",
    "        print(f\"Test:       {X_test[theme].mean():.3f}\")\n",
    "\n",
    "# Aufruf\n",
    "check_theme_distribution(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Letzter Vergleich der Modelle für alle Zielvariablen am Train-, Validierungs- und Test-Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Zusammenfassung aller Modelle (Train/Val/Test) ===\n",
      "                     Zielvariable  Train R²  Val R²  Test R²  Train RMSE  Val RMSE  Test RMSE  Train MAE  Val MAE  Test MAE                                                                                                                                                                  Features\n",
      "   Klickrate der Impressionen (%)    0.0438  0.0443   0.0216        2.29      2.15       2.21       1.77     1.75      1.75 [theme_Krieg, theme_Sonstiges, theme_Bilder, Bewertung_Titel, Gestaltung_Thumbnail, theme_Wirtschaft, theme_Politik, publish_weekday, publish_hour, video_length_seconds]\n",
      "durchschnittliche_wiedergabedauer    0.8642  0.6946   0.8471       35.03     43.76      33.62      22.38    22.27     23.00                [video_length_seconds, theme_Sonstiges, theme_Krieg, theme_Bilder, Bewertung_Titel, Gestaltung_Thumbnail, publish_hour, publish_weekday, theme_Wirtschaft]\n",
      "                          aufrufe    0.0478  0.0399   0.0733    90393.92 133565.28   79979.54   44625.83 50106.51  44775.76 [theme_Krieg, theme_Wirtschaft, publish_hour, Gestaltung_Thumbnail, publish_weekday, video_length_seconds, theme_Bilder, theme_Politik, theme_Sonstiges, Bewertung_Titel]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def main():\n",
    "    # 1) Ziele definieren: Du hast 3 Zielvariablen\n",
    "    target_vars = [\n",
    "        \"Klickrate der Impressionen (%)\",\n",
    "        \"durchschnittliche_wiedergabedauer\",\n",
    "        \"aufrufe\"\n",
    "    ]\n",
    "    \n",
    "    # 2) Daten laden (inkl. X_test, y_test)\n",
    "    with open('../data/processed/processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    X_test = data['X_test'].copy()\n",
    "    y_test_data = data['y_test']  # DataFrame oder Series mit allen Zielspalten\n",
    "    \n",
    "    # Falls 'publish_date' oder 'theme_Live' etc. noch aufbereitet werden muss, hier tun:\n",
    "    if 'publish_date' in X_test.columns:\n",
    "        X_test['publish_date'] = pd.to_datetime(X_test['publish_date'], format='%d.%m.%Y %H:%M')\n",
    "        X_test['publish_hour'] = X_test['publish_date'].dt.hour\n",
    "        X_test['publish_weekday'] = X_test['publish_date'].dt.dayofweek\n",
    "    \n",
    "    if 'theme_Live' in X_test.columns:\n",
    "        X_test.drop(columns=['theme_Live'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # 3) Ergebnisse sammeln\n",
    "    results_list = []  # Speichert alle Ergebnisse in einer Liste von Dicts\n",
    "    \n",
    "    for target in target_vars:\n",
    "        # a) JSON-Datei mit den Kennzahlen laden\n",
    "        json_path = f\"../models/best_model_{target}_info.json\"\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"Warnung: {json_path} nicht gefunden. Überspringe {target}.\")\n",
    "            continue\n",
    "        \n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as jf:\n",
    "            model_info = json.load(jf)\n",
    "        \n",
    "        # b) Modell laden (pkl-Datei)\n",
    "        model_path = f\"../models/best_model_{target}.pkl\"\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Warnung: {model_path} nicht gefunden. Überspringe {target}.\")\n",
    "            continue\n",
    "        \n",
    "        best_model = joblib.load(model_path)\n",
    "        \n",
    "        # c) Benötigte Features laut JSON\n",
    "        selected_features = model_info.get(\"selected_features\", [])\n",
    "        \n",
    "        # d) Test-Zielwerte extrahieren\n",
    "        if target not in y_test_data.columns:\n",
    "            print(f\"Zielvariable '{target}' nicht in y_test_data. Überspringe.\")\n",
    "            continue\n",
    "        y_test = y_test_data[target].copy()\n",
    "        y_test_str = data['y_test'][target].astype(str)\n",
    "        y_test_str = y_test_str.str.replace(\",\", \".\")\n",
    "        y_test = y_test_str.astype(float)\n",
    "\n",
    "        # e) X_test nach den im Modell verwendeten Features filtern\n",
    "        X_test_sub = X_test[selected_features].copy()\n",
    "        \n",
    "        # f) Vorhersage auf dem Testset\n",
    "        y_test_pred = best_model.predict(X_test_sub)\n",
    "        \n",
    "        # g) Kennzahlen berechnen\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        \n",
    "        # h) Vergleich mit Train/Val aus model_info\n",
    "        train_r2 = model_info.get(\"train_r2\", None)\n",
    "        val_r2   = model_info.get(\"val_r2\", None)\n",
    "        train_rmse = model_info.get(\"train_rmse\", None)\n",
    "        val_rmse   = model_info.get(\"val_rmse\", None)\n",
    "        train_mae  = model_info.get(\"train_mae\", None)\n",
    "        val_mae    = model_info.get(\"val_mae\", None)\n",
    "        \n",
    "        # i) Speichere Zusammenfassung in Dict\n",
    "        results_list.append({\n",
    "            \"Zielvariable\": target,\n",
    "            \"Train R²\": train_r2,\n",
    "            \"Val R²\": val_r2,\n",
    "            \"Test R²\": round(test_r2, 4),\n",
    "            \"Train RMSE\": train_rmse,\n",
    "            \"Val RMSE\": val_rmse,\n",
    "            \"Test RMSE\": round(test_rmse, 2),\n",
    "            \"Train MAE\": train_mae,\n",
    "            \"Val MAE\": val_mae,\n",
    "            \"Test MAE\": round(test_mae, 2),\n",
    "            \"Features\": selected_features\n",
    "        })\n",
    "    \n",
    "    # 4) Ausgabe in schöner Tabellenform\n",
    "    if results_list:\n",
    "        results_df = pd.DataFrame(results_list)\n",
    "        print(\"\\n=== Zusammenfassung aller Modelle (Train/Val/Test) ===\")\n",
    "        print(results_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"Keine Ergebnisse. Wurden die JSON- und pkl-Dateien gefunden?\")\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

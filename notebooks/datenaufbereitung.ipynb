{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenvorverarbereitung/Stichproben und Vor-Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zelle 1 - Setup und Imports:\n",
    "\n",
    "Importiert alle benötigten Bibliotheken\n",
    "Setzt grundlegende Darstellungsoptionen\n",
    "Konfiguriert den Plot-Stil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup und Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Setzen der Display-Optionen für bessere Lesbarkeit\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Warnung unterdrücken\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotstil festlegen (verwenden einen basic matplotlib style)\n",
    "plt.style.use('default')\n",
    "# Seaborn Plotting Style setzen\n",
    "sns.set_theme(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignorieren. Nur zum Testen des Verzechnisses\n",
    "import os\n",
    "print(\"Aktuelles Arbeitsverzeichnis:\", os.getcwd())\n",
    "print(\"\\nDateien im aktuellen Verzeichnis:\")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten laden und erste Inspektion:\n",
    "\n",
    "Lädt den Datensatz\n",
    "Zeigt grundlegende Informationen wie Dimensionen und Datentypen\n",
    "Überprüft auf fehlende Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 2: Daten laden und erste Inspektion\n",
    "# Daten einlesen mit zusätzlichen Parametern für robusteres Parsing\n",
    "# df = pd.read_csv('https://raw.githubusercontent.com/Takual/project/refs/heads/main/data/raw/videos_ab_60s.csv', \n",
    "#                 encoding='utf-8',\n",
    "#                 sep=';',  # Semikolon als Trennzeichen\n",
    "#                 decimal=',',  # Komma als Dezimaltrennzeichen\n",
    "#                 thousands='.',  # Punkt als Tausendertrennzeichen\n",
    "#                on_bad_lines='warn')  # Warnung bei problematischen Zeilen\n",
    "\n",
    "df = pd.read_csv('../data/raw/videos_ohnelive_ohneshorts.csv', encoding='utf-8', sep=';', decimal=',', thousands='.')\n",
    "\n",
    "# Überblick über den Datensatz\n",
    "print(\"Dimensionen des Datensatzes:\", df.shape)\n",
    "print(\"\\nInformationen über den Datensatz:\")\n",
    "print(df.info())\n",
    "\n",
    "# Erste Zeilen anzeigen\n",
    "print(\"\\nErste Zeilen des Datensatzes:\")\n",
    "print(df.head())\n",
    "\n",
    "# Überprüfung auf fehlende Werte\n",
    "print(\"\\nAnzahl fehlender Werte pro Spalte:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignore \n",
    "(Hier noch ein Überblick über die Spalten und die Datenstruktur, auftgeteilt in Gruppen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def show_column_groups_table(df):\n",
    "    def shorten_column_name(name, max_length=30):\n",
    "        \"\"\"Kürzt lange Spaltennamen sinnvoll\"\"\"\n",
    "        if len(name) <= max_length:\n",
    "            return name\n",
    "        \n",
    "        # Versuche, sinnvolle Abkürzungen zu erstellen\n",
    "        words = name.split('_')\n",
    "        if len(words) > 1:\n",
    "            # Nehme Anfangsbuchstaben und letzte Wörter\n",
    "            shortened = ''.join(word[0].upper() for word in words[:-1]) + '_' + words[-1]\n",
    "            return shortened[:max_length]\n",
    "        \n",
    "        # Für einen langen Namen ohne Unterstrich\n",
    "        return name[:15] + '...' + name[-10:]\n",
    "\n",
    "    # Gruppiere Spalten nach Datentyp\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    boolean_cols = df.select_dtypes(include=['bool']).columns.tolist()\n",
    "    \n",
    "    # Kürze Spaltennamen\n",
    "    numeric_cols_short = [shorten_column_name(col) for col in numeric_cols]\n",
    "    datetime_cols_short = [shorten_column_name(col) for col in datetime_cols]\n",
    "    categorical_cols_short = [shorten_column_name(col) for col in categorical_cols]\n",
    "    boolean_cols_short = [shorten_column_name(col) for col in boolean_cols]\n",
    "    \n",
    "    # Erstelle einen DataFrame für die Übersicht\n",
    "    column_groups = pd.DataFrame({\n",
    "        'Datentyp': [\n",
    "            'Numerische Spalten', \n",
    "            'Datetime Spalten', \n",
    "            'Kategorische/Text Spalten',\n",
    "            'Boolesche Spalten'\n",
    "        ],\n",
    "        'Spalten': [\n",
    "            ', '.join(numeric_cols_short) if numeric_cols_short else '-',\n",
    "            ', '.join(datetime_cols_short) if datetime_cols_short else '-',\n",
    "            ', '.join(categorical_cols_short) if categorical_cols_short else '-',\n",
    "            ', '.join(boolean_cols_short) if boolean_cols_short else '-'\n",
    "        ],\n",
    "        'Anzahl': [\n",
    "            len(numeric_cols),\n",
    "            len(datetime_cols),\n",
    "            len(categorical_cols),\n",
    "            len(boolean_cols)\n",
    "        ],\n",
    "        'Original Namen': [\n",
    "            ', '.join(numeric_cols) if numeric_cols else '-',\n",
    "            ', '.join(datetime_cols) if datetime_cols else '-',\n",
    "            ', '.join(categorical_cols) if categorical_cols else '-',\n",
    "            ', '.join(boolean_cols) if boolean_cols else '-'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Formatiere die Ausgabe\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(column_groups)\n",
    "    \n",
    "    # Zusätzliche Gesamtinformationen\n",
    "    print(f\"\\nGesamtanzahl Spalten: {len(df.columns)}\")\n",
    "    \n",
    "    return column_groups\n",
    "\n",
    "# Funktion aufrufen\n",
    "column_overview = show_column_groups_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datenbereinigung und Transformation:\n",
    "\n",
    "Konvertiert Datums-Strings in datetime-Objekte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 3: Datenbereinigung\n",
    "# Datumskonvertierung mit verbesserte Fehlerbehandlung\n",
    "if 'publish_date' in df.columns:\n",
    "    df['publish_date'] = pd.to_datetime(df['publish_date'], format='%d.%m.%Y %H:%M', errors='coerce')\n",
    "    df['publish_hour'] = df['publish_date'].dt.hour\n",
    "    df['publish_weekday'] = df['publish_date'].dt.dayofweek\n",
    "\n",
    "print(\"Datentypen nach Bereinigung:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "Diese Zelle bereitet die Features für die Modellierung vor. Ein wichtiger Schritt ist die Umwandlung der kategorialen Variable \"Thema\" in Dummy-Variablen oder Indikatorvariable. Dabei wird jede Kategorie (wie \"Politik\", \"Wirtschaft\" etc.) in eine eigene Spalte umgewandelt, die nur 0 und 1 enthält. Dies ist notwendig, um Multikollinarität zu vermeiden. Außerdem werden hier die Ziel- und Prädiktorvariablen definiert, die für die späteren Modelle verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 4: Feature Engineering\n",
    "# Dummy-Variablen für Thema erstellen\n",
    "if 'Thema' in df.columns:\n",
    "    theme_dummies = pd.get_dummies(df['Thema'], prefix='theme')\n",
    "    df = pd.concat([df, theme_dummies], axis=1)\n",
    "\n",
    "# Zielvariablen definieren\n",
    "target_columns = ['durchschnittliche_wiedergabedauer', 'aufrufe', 'Klickrate der Impressionen (%)']\n",
    "\n",
    "# Features für die Modellierung auswählen\n",
    "feature_columns = [col for col in df.columns if col not in target_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skalierung der Features\n",
    "In dieser Zelle werden die numerischen Features standardisiert. Dies ist wichtig, weil die verschiedenen Features sehr unterschiedliche Größenordnungen haben (z.B. Video-Länge in Sekunden vs. Bewertungen von 1-10). Die Standardisierung sorgt dafür, dass alle numerischen Features einen Mittelwert von 0 und eine Standardabweichung von 1 haben. Dies verhindert, dass Features mit großen Zahlen das Modell unverhältnismäßig stark beeinflussen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 5: Selektive Skalierung der Features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaler für bestimmte numerische Features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Spalten, die skaliert werden sollten (große Wertebereiche)\n",
    "columns_to_scale = [\n",
    "    'video_length_seconds', 'wiedergabezeit_minuten', 'durchschnittliche_wiedergabedauer',\n",
    "    'aufrufe', 'likes', 'dislikes', 'geteilte_inhalte', 'kommentare', 'Impressionen'\n",
    "]\n",
    "\n",
    "# Skalieren nur dieser Spalten\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "print(\"\\nFeature-Columns:\", feature_columns)\n",
    "print(\"\\nNumerische Features, die skaliert wurden:\", columns_to_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test-Split\n",
    "Diese Zelle teilt die Daten in drei Teile auf: Trainings-, Validierungs- und Testdaten. Der Trainingsdatensatz (60%) wird verwendet, um das Modell zu trainieren. Der Validierungsdatensatz (20%) dient dazu, die Modellleistung während des Trainings zu überprüfen und  zu optimieren. Der Testdatensatz (20%) wird erst ganz am Ende verwendet, um die finale Modellperformance zu evaluieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 6: Train-Test-Split\n",
    "# Features und Targets separieren\n",
    "X = df[feature_columns]\n",
    "y = df[target_columns]\n",
    "\n",
    "# Erste Aufteilung in temporäres Training+Validierung und Test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Zweite Aufteilung des temporären Sets in Training und Validierung\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42  # 0.25 x 0.8 = 0.2\n",
    ")\n",
    "\n",
    "print(\"\\nDimensionen der Datensätze:\")\n",
    "print(f\"Training: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validierung: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daten speichern\n",
    "In der letzten Zelle werden alle verarbeiteten Daten gespeichert, damit sie in späteren Analysen wieder verwendet werden können. Die Daten werden sowohl im Pickle-Format (für Python-spezifische Objekte wie den Scaler) als auch als CSV-Dateien gespeichert. CSVs sind dabei besonders nützlich für schnelle Inspektionen der Daten. Wichtig ist auch, dass der Scaler mitgespeichert wird, damit neue Daten auf die gleiche Weise transformiert werden können.\n",
    "Dies stellt sicher, dass wir einen sauberen, reproduzierbaren Datensatz für unsere Analysen haben und dass alle Transformationen konsistent auf zukünftige Daten angewendet werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Daten wurden erfolgreich gespeichert in 'data/processed/processed_data.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Zelle 7: Daten speichern\n",
    "# Ordner für verarbeitete Daten erstellen\n",
    "if not os.path.exists('data/processed'):\n",
    "    os.makedirs('data/processed')\n",
    "\n",
    "# Daten als Pickle-Datei speichern\n",
    "with open('../data/processed/processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test,\n",
    "        'scaler': scaler,\n",
    "        'feature_columns': feature_columns,\n",
    "        'target_columns': target_columns\n",
    "    }, f)\n",
    "\n",
    "print(\"\\nDaten wurden erfolgreich gespeichert in 'data/processed/processed_data.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Auch als CSV speichern für einfachere Inspektion\n",
    "X_train.to_csv('../data/processed/X_train.csv', sep=';', decimal=',', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', sep=';', decimal=',', index=False)\n",
    "X_val.to_csv('../data/processed/X_val.csv', sep=';', decimal=',', index=False)\n",
    "y_val.to_csv('../data/processed/y_val.csv', sep=';', decimal=',', index=False)\n",
    "X_test.to_csv('../data/processed/X_test.csv', sep=';', decimal=',', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', sep=';', decimal=',', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

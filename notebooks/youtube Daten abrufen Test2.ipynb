{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install isodate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code zum Abrufen fast aller Daten des youtube Channels. Es müssen noch die API-Daten und der Link zur json-Datei eingetragen werden. Außerdem der Datumsbereich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Konfiguration - Hier Ihre Werte eintragen\n",
    "CHANNEL_ID = \"UCSeil5V81-mEGB1-VNR7YEA\"  # Ihre YouTube Channel ID\n",
    "DATA_API_V3_KEY = \"\"  # API Key für YouTube Data API v3\n",
    "#ANALYTICS_API_KEY = \"\"  # API Key für YouTube Analytics API\n",
    "CREDENTIALS_PATH = \"\"\n",
    "\n",
    "\n",
    "# Datum Konfiguration - Format: \"DD.MM.YYYY\"\n",
    "START_DATE = \"01.01.2024\"  # z.B. \"01.09.2024\"\n",
    "END_DATE = \"31.01.2024\"   # z.B. \"02.09.2024\"\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import isodate\n",
    "\n",
    "# OAuth 2.0 Scopes\n",
    "SCOPES = ['https://www.googleapis.com/auth/yt-analytics.readonly']\n",
    "\n",
    "def get_video_data_from_api():\n",
    "    \"\"\"YouTube Data API v3 Abfrage für Basis-Videodaten\"\"\"\n",
    "    youtube = build('youtube', 'v3', developerKey=DATA_API_V3_KEY)\n",
    "    \n",
    "    start_date = datetime.strptime(START_DATE, '%d.%m.%Y')\n",
    "    end_date = datetime.strptime(END_DATE, '%d.%m.%Y').replace(hour=23, minute=59, second=59)\n",
    "    \n",
    "    start_date_rfc = start_date.isoformat() + 'Z'\n",
    "    end_date_rfc = end_date.isoformat() + 'Z'\n",
    "    \n",
    "    videos_data = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    while True:\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            channelId=CHANNEL_ID,\n",
    "            maxResults=50,\n",
    "            order='date',\n",
    "            publishedAfter=start_date_rfc,\n",
    "            publishedBefore=end_date_rfc,\n",
    "            type='video',\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        \n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response['items']:\n",
    "            video_id = item['id']['videoId']\n",
    "            \n",
    "            video_request = youtube.videos().list(\n",
    "                part='snippet,contentDetails,status',\n",
    "                id=video_id\n",
    "            )\n",
    "            video_response = video_request.execute()\n",
    "            \n",
    "            if video_response['items']:\n",
    "                video_info = video_response['items'][0]\n",
    "                duration = int(isodate.parse_duration(video_info['contentDetails']['duration']).total_seconds())\n",
    "                \n",
    "                video_data = {\n",
    "                    'video_id': video_id,\n",
    "                    'title': video_info['snippet']['title'],\n",
    "                    'publish_date': video_info['snippet']['publishedAt'],\n",
    "                    'video_length_seconds': duration,\n",
    "                    'privacy_status': video_info['status']['privacyStatus']\n",
    "                }\n",
    "                videos_data.append(video_data)\n",
    "        \n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(videos_data)\n",
    "\n",
    "def get_analytics_data(analytics, video_id, start_date, end_date):\n",
    "    \"\"\"YouTube Analytics API Abfrage für detaillierte Metriken\"\"\"\n",
    "    try:\n",
    "        # Performance Metriken\n",
    "        metrics = [\n",
    "            'estimatedMinutesWatched',\n",
    "            'averageViewDuration',\n",
    "            'averageViewPercentage',\n",
    "            'subscribersGained',\n",
    "            'subscribersLost',\n",
    "            'views',\n",
    "            'likes',\n",
    "            'dislikes',\n",
    "            'shares',\n",
    "            'comments'\n",
    "        ]\n",
    "        \n",
    "        perf_response = analytics.reports().query(\n",
    "            ids=f'channel=={CHANNEL_ID}',\n",
    "            startDate=start_date.strftime('%Y-%m-%d'),\n",
    "            endDate=end_date.strftime('%Y-%m-%d'),\n",
    "            metrics=','.join(metrics),\n",
    "            filters=f'video=={video_id}'\n",
    "        ).execute()\n",
    "\n",
    "        # Demografische Daten\n",
    "        demo_response = analytics.reports().query(\n",
    "            ids=f'channel=={CHANNEL_ID}',\n",
    "            startDate=start_date.strftime('%Y-%m-%d'),\n",
    "            endDate=end_date.strftime('%Y-%m-%d'),\n",
    "            metrics='viewerPercentage',\n",
    "            dimensions='ageGroup,gender',\n",
    "            filters=f'video=={video_id}'\n",
    "        ).execute()\n",
    "\n",
    "        # Basis-Daten sammeln\n",
    "        data = {\n",
    "            'video_id': video_id,\n",
    "            'wiedergabezeit_minuten': 0,\n",
    "            'durchschnittliche_wiedergabedauer': 0,\n",
    "            'durchschnittliche_wiedergabedauer_prozent': 0,\n",
    "            'gewonnene_abonnenten': 0,\n",
    "            'verlorene_abonnenten': 0,\n",
    "            'aufrufe': 0,\n",
    "            'likes': 0,\n",
    "            'dislikes': 0,\n",
    "            'geteilte_inhalte': 0,\n",
    "            'kommentare': 0\n",
    "        }\n",
    "\n",
    "        # Performance Daten einfügen\n",
    "        if 'rows' in perf_response and perf_response['rows']:\n",
    "            row = perf_response['rows'][0]\n",
    "            data.update({\n",
    "                'wiedergabezeit_minuten': row[0],\n",
    "                'durchschnittliche_wiedergabedauer': row[1],\n",
    "                'durchschnittliche_wiedergabedauer_prozent': row[2],\n",
    "                'gewonnene_abonnenten': row[3],\n",
    "                'verlorene_abonnenten': row[4],\n",
    "                'aufrufe': row[5],\n",
    "                'likes': row[6],\n",
    "                'dislikes': row[7],\n",
    "                'geteilte_inhalte': row[8],\n",
    "                'kommentare': row[9]\n",
    "            })\n",
    "\n",
    "        # Demografische Daten einfügen\n",
    "        if 'rows' in demo_response:\n",
    "            for row in demo_response['rows']:\n",
    "                age_group, gender, percentage = row\n",
    "                key = f\"audience_{gender.lower()}_{age_group}\"\n",
    "                data[key] = percentage\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei Video {video_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"1. Hole Basis-Videodaten über YouTube Data API v3...\")\n",
    "        video_df = get_video_data_from_api()\n",
    "        print(f\"   - {len(video_df)} Videos gefunden\")\n",
    "\n",
    "        print(\"\\n2. Hole Analytics-Daten über YouTube Analytics API...\")\n",
    "        # Authentifizierung für Analytics API\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)\n",
    "        credentials = flow.run_local_server(port=0)\n",
    "        analytics = build('youtubeAnalytics', 'v2', credentials=credentials)\n",
    "        \n",
    "        start_date = datetime.strptime(START_DATE, '%d.%m.%Y')\n",
    "        end_date = datetime.strptime(END_DATE, '%d.%m.%Y')\n",
    "        \n",
    "        # Sammle Analytics-Daten für jedes Video\n",
    "        analytics_data = []\n",
    "        for index, row in video_df.iterrows():\n",
    "            print(f\"   - Verarbeite Video {index + 1} von {len(video_df)}: {row['video_id']}\")\n",
    "            data = get_analytics_data(analytics, row['video_id'], start_date, end_date)\n",
    "            if data:\n",
    "                analytics_data.append(data)\n",
    "\n",
    "        # Erstelle DataFrame aus Analytics-Daten\n",
    "        analytics_df = pd.DataFrame(analytics_data)\n",
    "        \n",
    "        # Merge beide DataFrames\n",
    "        final_df = pd.merge(video_df, analytics_df, on='video_id', how='left')\n",
    "        \n",
    "        # Speichere Ergebnis und stelle sicher, dass Videolänge als Ganzzahl gespeichtert wird\n",
    "        video_df['video_length_seconds'] = video_df['video_length_seconds'].astype(int)\n",
    "        output_filename = f'youtube_complete_analysis_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "        \n",
    "        print(f\"\\nAnalyse abgeschlossen!\")\n",
    "        print(f\"Daten wurden in {output_filename} gespeichert\")\n",
    "        print(f\"Anzahl der Videos: {len(final_df)}\")\n",
    "        print(f\"Anzahl der Spalten: {len(final_df.columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ein Fehler ist aufgetreten: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird eine Liste von video_ids abgefragt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Konfiguration - Hier Ihre Werte eintragen\n",
    "CHANNEL_ID = \"UCSeil5V81-mEGB1-VNR7YEA\"  # Ihre YouTube Channel ID\n",
    "DATA_API_V3_KEY = \"\"  # API Key für YouTube Data API v3\n",
    "#ANALYTICS_API_KEY = \"AIzaSyCBw3tVk-TnYLCZW2vmojQlYFBArzudU-A\"  # API Key für YouTube Analytics API\n",
    "CREDENTIALS_PATH = \"C:\\\\Users\\\\laukat\\\\OneDrive - Mediengruppe RTL\\\\HDM Data Analyti\\\\oauth 2.0\\\\client_secret_796311161257-bnk32mvms5t9fsfma7agbgrlt5fo54gi.apps.googleusercontent.com.json\"  # Pfad zur client_secrets.json\"\n",
    "\n",
    "\n",
    "# Datum Konfiguration - Format: \"DD.MM.YYYY\"\n",
    "START_DATE = \"01.01.2000\"\n",
    "END_DATE = \"31.01.2100\"\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import isodate\n",
    "\n",
    "# OAuth 2.0 Scopes\n",
    "SCOPES = ['https://www.googleapis.com/auth/yt-analytics.readonly']\n",
    "\n",
    "def get_video_data_from_list(video_ids):\n",
    "    \"\"\"Hole Videodaten basierend auf einer Liste von Video-IDs\"\"\"\n",
    "    youtube = build('youtube', 'v3', developerKey=DATA_API_V3_KEY)\n",
    "    \n",
    "    videos_data = []\n",
    "    for video_id in video_ids:\n",
    "        video_request = youtube.videos().list(\n",
    "            part='snippet,contentDetails,status',\n",
    "            id=video_id\n",
    "        )\n",
    "        video_response = video_request.execute()\n",
    "        \n",
    "        if video_response['items']:\n",
    "            video_info = video_response['items'][0]\n",
    "            duration = int(isodate.parse_duration(video_info['contentDetails']['duration']).total_seconds())\n",
    "            \n",
    "            video_data = {\n",
    "                'video_id': video_id,\n",
    "                'title': video_info['snippet']['title'],\n",
    "                'publish_date': video_info['snippet']['publishedAt'],\n",
    "                'video_length_seconds': duration,\n",
    "                'privacy_status': video_info['status']['privacyStatus']\n",
    "            }\n",
    "            videos_data.append(video_data)\n",
    "    \n",
    "    return pd.DataFrame(videos_data)\n",
    "\n",
    "def get_analytics_data(analytics, video_id, start_date, end_date):\n",
    "    \"\"\"YouTube Analytics API Abfrage für detaillierte Metriken\"\"\"\n",
    "    # Unverändert vom Original\n",
    "    ...\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"1. Lade Video-IDs aus der bereinigten Datei...\")\n",
    "        cleaned_video_df = pd.read_csv(\"cleaned_datenpaket2.csv\")\n",
    "        video_ids = cleaned_video_df['video_id'].tolist()\n",
    "        print(f\"   - {len(video_ids)} Video-IDs geladen\")\n",
    "        \n",
    "        print(\"\\n2. Hole Basis-Videodaten über YouTube Data API v3...\")\n",
    "        video_df = get_video_data_from_list(video_ids)\n",
    "        print(f\"   - {len(video_df)} Videodaten abgerufen\")\n",
    "\n",
    "        print(\"\\n3. Hole Analytics-Daten über YouTube Analytics API...\")\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)\n",
    "        credentials = flow.run_local_server(port=0)\n",
    "        analytics = build('youtubeAnalytics', 'v2', credentials=credentials)\n",
    "        \n",
    "        # Start- und Enddatum festlegen\n",
    "        start_date = datetime.strptime(START_DATE, '%d.%m.%Y')\n",
    "        end_date = datetime.strptime(END_DATE, '%d.%m.%Y')\n",
    "\n",
    "        # Analytics-Daten sammeln\n",
    "        analytics_data = []\n",
    "        for index, row in video_df.iterrows():\n",
    "            print(f\"   - Verarbeite Video {index + 1} von {len(video_df)}: {row['video_id']}\")\n",
    "            data = get_analytics_data(analytics, row['video_id'], start_date, end_date)\n",
    "            if data:\n",
    "                analytics_data.append(data)\n",
    "\n",
    "        analytics_df = pd.DataFrame(analytics_data)\n",
    "        \n",
    "        # Merge original cleaned file with API results\n",
    "        final_df = pd.merge(cleaned_video_df, video_df, on='video_id', how='left')  # Basisdaten\n",
    "        final_df = pd.merge(final_df, analytics_df, on='video_id', how='left')     # Analytics-Daten\n",
    "\n",
    "        output_filename = f'youtube_analysis_with_cleaned_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "\n",
    "        print(f\"\\nAnalyse abgeschlossen!\")\n",
    "        print(f\"Daten wurden in {output_filename} gespeichert\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ein Fehler ist aufgetreten: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Folgende Code ruft bestimmte video_ids ab, die in einer cs gespeichert sind.\n",
    "Der Code ist so eingerichtet, dass er:\n",
    "\n",
    "Kontinuierlich in die CSV-Datei speichert\n",
    "Einen Checkpoint für den letzten verarbeiteten Index anlegt\n",
    "Fortschrittsinformationen ausgibt.\n",
    "Der Code kann unterbrochen werden und arbeitet bei Wiederaufnahme an der letzten Stelle fort. \n",
    "Weitere Merkmale:\n",
    "Explizite Float-Konvertierung der Prozentwerte\n",
    "Mehr Debug-Ausgaben um zu sehen, was genau passiert\n",
    "Verhinderung der String-Formatierung beim Speichern\n",
    "Verifikation der gespeicherten Daten\n",
    "Verwendung von Komma als Dezimaltrennzeichen\n",
    "Semikolon als Spaltentrenner\n",
    "Formatierung aller numerischen Werte auf 2 Nachkommastellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Konfiguration - Hier Ihre Werte eintragen\n",
    "CHANNEL_ID = \"UCSeil5V81-mEGB1-VNR7YEA\"  # Ihre YouTube Channel ID\n",
    "DATA_API_V3_KEY = \"AIzaSyCBw3tVk-TnYLCZW2vmojQlYFBArzudU-A\"  # API Key für YouTube Data API v3\n",
    "CREDENTIALS_PATH = \"C:\\\\Users\\\\laukat\\\\OneDrive - Mediengruppe RTL\\\\HDM Data Analyti\\\\oauth 2.0\\\\client_secret_796311161257-bnk32mvms5t9fsfma7agbgrlt5fo54gi.apps.googleusercontent.com.json\"\n",
    "\n",
    "# Datum Konfiguration\n",
    "START_DATE = \"01.01.2000\"\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# OAuth 2.0 Scopes\n",
    "SCOPES = ['https://www.googleapis.com/auth/yt-analytics.readonly']\n",
    "\n",
    "# Konstanten\n",
    "CHECKPOINT_FILE = 'analytics_checkpoint.json'\n",
    "OUTPUT_FILE = 'youtube_demographic_analysis_continuous.csv'\n",
    "\n",
    "def read_csv_with_encoding(file_path):\n",
    "    \"\"\"Versucht die CSV-Datei mit verschiedenen Encodings zu lesen\"\"\"\n",
    "    encodings = ['utf-8', 'iso-8859-1', 'cp1252', 'latin1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"Versuche Encoding: {encoding}\")\n",
    "            return pd.read_csv(file_path, encoding=encoding, sep=';')\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Lesen der Datei mit {encoding}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    raise Exception(\"Konnte die Datei mit keinem der verfügbaren Encodings lesen\")\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"Speichert oder aktualisiert die CSV-Datei mit deutschen Zahlenformat\"\"\"\n",
    "    try:\n",
    "        # Wenn Datei existiert, lade bestehende Daten\n",
    "        if os.path.exists(filename):\n",
    "            df_existing = pd.read_csv(filename, sep=';', decimal=',')\n",
    "            df_new = pd.DataFrame([data])\n",
    "            df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        else:\n",
    "            # Erste Daten\n",
    "            df_combined = pd.DataFrame([data])\n",
    "        \n",
    "        # Formatiere alle numerischen Spalten\n",
    "        for col in df_combined.columns:\n",
    "            if col != 'video_id':\n",
    "                # Konvertiere zu numerisch falls noch nicht geschehen\n",
    "                df_combined[col] = pd.to_numeric(df_combined[col], errors='ignore')\n",
    "                # Formatiere mit deutschem Format (Komma statt Punkt)\n",
    "                df_combined[col] = df_combined[col].apply(lambda x: f\"{x:.2f}\".replace('.', ','))\n",
    "        \n",
    "        # Speichere mit Semikolon als Trenner\n",
    "        df_combined.to_csv(filename, sep=';', index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(\"\\nDaten erfolgreich gespeichert\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Speichern: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def get_gender_age_analytics(analytics, video_id, start_date, end_date):\n",
    "    \"\"\"YouTube Analytics API Abfrage für Geschlechter- und Altersverteilung\"\"\"\n",
    "    try:\n",
    "        request = analytics.reports().query(\n",
    "            dimensions='ageGroup,gender',\n",
    "            ids=f'channel=={CHANNEL_ID}',\n",
    "            metrics='viewerPercentage',\n",
    "            filters=f'video=={video_id}',\n",
    "            startDate=start_date.strftime('%Y-%m-%d'),\n",
    "            endDate=end_date.strftime('%Y-%m-%d')\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Initialisiere Dictionary mit allen möglichen Altersgruppen\n",
    "        age_groups = ['13-17', '18-24', '25-34', '35-44', '45-54', '55-64', '65-']\n",
    "        data = {\n",
    "            'video_id': video_id\n",
    "        }\n",
    "        for age in age_groups:\n",
    "            data[f'audience_female_age{age}'] = 0.0  # Explizit als float\n",
    "            data[f'audience_male_age{age}'] = 0.0    # Explizit als float\n",
    "            \n",
    "        value_count = 0\n",
    "        total_percentage = 0\n",
    "            \n",
    "        if 'rows' in response:\n",
    "            print(f\"\\nGefundene Werte für Video {video_id}:\")\n",
    "            for row in response['rows']:\n",
    "                age_group = row[0]\n",
    "                gender = row[1]\n",
    "                percentage = float(row[2])  # Explizite Konvertierung zu float\n",
    "                \n",
    "                column_name = f'audience_{gender}_age{age_group}'\n",
    "                data[column_name] = percentage\n",
    "                print(f\"{column_name}: {percentage:.1f}%\")\n",
    "                \n",
    "                value_count += 1\n",
    "                total_percentage += percentage\n",
    "            \n",
    "            print(f\"\\nDatenqualität:\")\n",
    "            print(f\"Anzahl gefundener Werte: {value_count}\")\n",
    "            print(f\"Summe der Prozente: {total_percentage:.1f}%\")\n",
    "            \n",
    "            if value_count < 5:\n",
    "                print(\"Zu wenige demografische Daten vorhanden - Datensatz wird übersprungen\")\n",
    "                return None\n",
    "                \n",
    "            if total_percentage < 20:\n",
    "                print(\"Gesamtprozentsatz zu niedrig - Datensatz wird übersprungen\")\n",
    "                return None\n",
    "            \n",
    "            # Debug-Ausgabe der finalen Daten\n",
    "            print(\"\\nFinale Daten vor dem Speichern:\")\n",
    "            for key, value in data.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "                \n",
    "            return data\n",
    "        else:\n",
    "            print(f\"\\nKeine Daten für Video {video_id} gefunden\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        if 'quota' in str(e).lower():\n",
    "            print(f\"Quota-Limit erreicht bei Video {video_id}\")\n",
    "            raise e\n",
    "        print(f\"Fehler bei Video {video_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_videos(analytics, video_ids, start_date, end_date, start_index=0):\n",
    "    \"\"\"Verarbeitet Videos und zeigt Fortschritt\"\"\"\n",
    "    total_videos = len(video_ids)\n",
    "    start_time = time.time()\n",
    "    valid_data_count = 0\n",
    "    skipped_data_count = 0\n",
    "    \n",
    "    try:\n",
    "        for index, video_id in enumerate(video_ids[start_index:], start=start_index):\n",
    "            current_time = time.time()\n",
    "            videos_processed = index - start_index + 1\n",
    "            \n",
    "            if videos_processed > 0:\n",
    "                time_per_video = (current_time - start_time) / videos_processed\n",
    "                videos_remaining = total_videos - (index + 1)\n",
    "                estimated_time_remaining = timedelta(seconds=int(videos_remaining * time_per_video))\n",
    "                \n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Video {index + 1} von {total_videos}: {video_id}\")\n",
    "                print(f\"Durchschnittliche Zeit pro Video: {time_per_video:.1f} Sekunden\")\n",
    "                print(f\"Geschätzte verbleibende Zeit: {estimated_time_remaining}\")\n",
    "                print(f\"Bisher valide Datensätze: {valid_data_count}\")\n",
    "                print(f\"Übersprungene Datensätze: {skipped_data_count}\")\n",
    "            \n",
    "            data = get_gender_age_analytics(analytics, video_id, start_date, end_date)\n",
    "            if data:\n",
    "                if save_to_csv(data, OUTPUT_FILE):\n",
    "                    valid_data_count += 1\n",
    "                    print(f\"Daten in {OUTPUT_FILE} gespeichert\")\n",
    "            else:\n",
    "                skipped_data_count += 1\n",
    "            \n",
    "            # Speichere Checkpoint\n",
    "            with open(CHECKPOINT_FILE, 'w') as f:\n",
    "                json.dump({'last_processed_index': index}, f)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler aufgetreten: {str(e)}\")\n",
    "        print(f\"Letzter verarbeiteter Index: {index}\")\n",
    "        raise e\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Lade Video-IDs mit korrektem Encoding\n",
    "        print(\"Lade Video-IDs aus der bereinigten Datei...\")\n",
    "        video_df = read_csv_with_encoding(\"daten bereinigt dezimalzeichen fehlt in gender.csv\")\n",
    "        video_ids = video_df['video_id'].tolist()\n",
    "        print(f\"Geladen: {len(video_ids)} Videos\")\n",
    "        \n",
    "        # Lade letzten Fortschritt\n",
    "        start_index = -1\n",
    "        if os.path.exists(CHECKPOINT_FILE):\n",
    "            with open(CHECKPOINT_FILE, 'r') as f:\n",
    "                start_index = json.load(f)['last_processed_index']\n",
    "        \n",
    "        # Initialisiere API\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)\n",
    "        credentials = flow.run_local_server(port=0)\n",
    "        analytics = build('youtubeAnalytics', 'v2', credentials=credentials)\n",
    "        \n",
    "        # Setze Datumsgrenzen\n",
    "        start_date = datetime.strptime(START_DATE, '%d.%m.%Y')\n",
    "        end_date = datetime.now()\n",
    "        \n",
    "        print(f\"Starte Verarbeitung ab Index {start_index + 1}\")\n",
    "        process_videos(analytics, video_ids, start_date, end_date, start_index + 1)\n",
    "        \n",
    "        print(\"\\nVerarbeitung abgeschlossen!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler: {str(e)}\")\n",
    "        print(\"Sie können das Skript später neu starten - es wird am letzten Checkpoint fortgesetzt.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anbei ein Code, um die Spalte Thema per \"Hand\" anzupassen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import msvcrt  # Für Windows-Konsoleneingabe\n",
    "\n",
    "def get_key():\n",
    "    \"\"\"Wartet auf einen Tastendruck und gibt den Wert zurück\"\"\"\n",
    "    return msvcrt.getch().decode('utf-8')\n",
    "\n",
    "def categorize_videos():\n",
    "    # Prüfen ob die Datei existiert\n",
    "    if not os.path.exists('updated_allewerte.csv'):\n",
    "        print(\"FEHLER: youtube_data.csv nicht gefunden!\")\n",
    "        print(\"Bitte legen Sie die Datei im gleichen Ordner ab wie das Python-Skript\")\n",
    "        input(\"Drücken Sie Enter zum Beenden...\")\n",
    "        return\n",
    "\n",
    "    # CSV einlesen mit Semikolon als Trennzeichen und Komma als Dezimalzeichen\n",
    "    print(\"Lade Datei...\")\n",
    "    df = pd.read_csv('updated_allewerte.csv', sep=';', decimal=',')\n",
    "    \n",
    "    # Kategorien definieren\n",
    "    categories = ['Politik', 'Krieg', 'Wirtschaft', 'Bilder', 'Sonstiges']\n",
    "    \n",
    "    print(\"\\nKategorisierung beginnt...\")\n",
    "    print(\"Steuerung:\")\n",
    "    print(\"1-5: Kategorie auswählen\")\n",
    "    print(\"0: Video überspringen\")\n",
    "    print(\"q: Programm beenden und speichern\")\n",
    "    input(\"Drücken Sie Enter zum Starten...\")\n",
    "\n",
    "    # Durchgehen jeder Zeile, die nicht \"Live\" als Thema hat\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Thema'] != 'Live':\n",
    "            os.system('cls')  # Bildschirm löschen\n",
    "            print(f\"\\nVideo {index + 1} von {len(df)}\")\n",
    "            print('-' * 50)\n",
    "            print('Titel:', row['title'])\n",
    "            print('Aktuelles Thema:', row['Thema'])\n",
    "            print('\\nKategorien:')\n",
    "            for i, category in enumerate(categories, 1):\n",
    "                print(f'{i}: {category}')\n",
    "            print('0: Überspringen')\n",
    "            print('q: Beenden und Speichern')\n",
    "            \n",
    "            while True:\n",
    "                print(\"\\nWählen Sie eine Kategorie (1-5) oder 0 zum Überspringen: \")\n",
    "                key = get_key()\n",
    "                \n",
    "                if key == 'q':\n",
    "                    # Speichern und beenden\n",
    "                    df.to_csv('updated_allewerte.csv', sep=';', decimal=',', index=False)\n",
    "                    print(\"\\nÄnderungen wurden gespeichert in 'youtube_data_updated.csv'\")\n",
    "                    input(\"Drücken Sie Enter zum Beenden...\")\n",
    "                    return\n",
    "                \n",
    "                if key in ['0', '1', '2', '3', '4', '5']:\n",
    "                    if key != '0':\n",
    "                        df.at[index, 'Thema'] = categories[int(key)-1]\n",
    "                        print(f\"Kategorie geändert zu: {categories[int(key)-1]}\")\n",
    "                    break\n",
    "                \n",
    "    # Speichern der aktualisierten CSV\n",
    "    df.to_csv('updated_allewerte1.csv', sep=';', decimal=',', index=False)\n",
    "    print('\\nKategorisierung abgeschlossen!')\n",
    "    print('Datei wurde gespeichert als: updated_allewerte1.csv')\n",
    "    input(\"Drücken Sie Enter zum Beenden...\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        categorize_videos()\n",
    "    except Exception as e:\n",
    "        print(f\"Ein Fehler ist aufgetreten: {str(e)}\")\n",
    "        input(\"Drücken Sie Enter zum Beenden...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Code analysietr alle Thumbnails der Videos und bewertet sie nach Kriterien wie präsente Personen + Text (3), Präsenter Text (2) nur Screenshot (1), Erstmal nur zum testen 10 zufällige Videos. \n",
    "\n",
    "Nur Gesichter mit hoher Erkennungssicherheit (> 0.8)\n",
    "Mindestgröße der Gesichter im Bild\n",
    "Überprüfung der Position im Bild\n",
    "\n",
    "\n",
    "Strengere Kriterien für Text:\n",
    "\n",
    "Text muss mindestens 10% der Bildfläche einnehmen\n",
    "Prüfung der Textposition und -größe\n",
    "\n",
    "\n",
    "Neue Visualisierung:\n",
    "\n",
    "Erstellt einen HTML-Bericht mit allen Thumbnails\n",
    "Farbkodierung:\n",
    "\n",
    "Rot = Screenshot only\n",
    "Gelb = Screenshot mit Text\n",
    "Grün = Person im Fokus\n",
    "\n",
    "\n",
    "Zeigt alle relevanten Analyseergebnisse neben jedem Thumbnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-cloud-vision pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from google.cloud import vision\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Konstanten\n",
    "CREDENTIALS_PATH = r\"C:\\Users\\Laukat\\OneDrive - Mediengruppe RTL\\HDM Data Analyti\\Google cloud vision API\\youtube-analyse-444120-8ce0b527c6e8.json\"\n",
    "CSV_PATH = r\"C:\\Users\\Laukat\\OneDrive - Mediengruppe RTL\\HDM Data Analyti\\project\\data\\raw\\3_update_allewerte_Themenkategoristiert_dopplungen_bereinigt.csv\"\n",
    "OUTPUT_DIR = \"youtube_thumbnails\"\n",
    "ANALYSIS_OUTPUT = \"thumbnail_analysis_sample.csv\"\n",
    "SAMPLE_SIZE = 10\n",
    "\n",
    "class ThumbnailCategory(Enum):\n",
    "    SCREENSHOT_ONLY = 1\n",
    "    SCREENSHOT_WITH_TEXT = 2\n",
    "    PERSON_FOCUSED = 3\n",
    "\n",
    "class YouTubeThumbnailProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialisiert den Processor mit Google Vision Client\"\"\"\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = CREDENTIALS_PATH\n",
    "        self.vision_client = vision.ImageAnnotatorClient()\n",
    "        \n",
    "        if not os.path.exists(OUTPUT_DIR):\n",
    "            os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "    def download_thumbnail(self, video_id: str, quality: str = \"maxresdefault\") -> Optional[str]:\n",
    "        \"\"\"Lädt ein einzelnes Thumbnail herunter\"\"\"\n",
    "        base_url = \"https://img.youtube.com/vi/\"\n",
    "        thumbnail_url = urljoin(base_url, f\"{video_id}/{quality}.jpg\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(thumbnail_url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            output_path = os.path.join(OUTPUT_DIR, f\"{video_id}.jpg\")\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "                \n",
    "            print(f\"Erfolgreich heruntergeladen: {video_id}\")\n",
    "            return output_path\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Fehler beim Herunterladen von {video_id}: {str(e)}\")\n",
    "            if quality == \"maxresdefault\":\n",
    "                print(\"Versuche mit niedrigerer Qualität (hqdefault)...\")\n",
    "                return self.download_thumbnail(video_id, \"hqdefault\")\n",
    "            return None\n",
    "\n",
    "    def analyze_image(self, image_path: str) -> dict:\n",
    "        \"\"\"Analysiert ein Thumbnail mit Google Vision API\"\"\"\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        \n",
    "        image = vision.Image(content=content)\n",
    "        \n",
    "        # Einen einzelnen Aufruf mit allen Features machen\n",
    "        features = [\n",
    "            vision.Feature(type_=vision.Feature.Type.FACE_DETECTION),\n",
    "            vision.Feature(type_=vision.Feature.Type.LABEL_DETECTION),\n",
    "            vision.Feature(type_=vision.Feature.Type.TEXT_DETECTION)\n",
    "        ]\n",
    "        request = vision.AnnotateImageRequest(image=image, features=features)\n",
    "        response = self.vision_client.annotate_image(request=request)\n",
    "        \n",
    "        # Personen-Analyse mit strengeren Kriterien\n",
    "        faces = response.face_annotations\n",
    "        prominent_faces = []\n",
    "        for face in faces:\n",
    "            # Berechne die Gesichtsgröße\n",
    "            bbox = face.bounding_poly.vertices\n",
    "            face_width = abs(bbox[1].x - bbox[0].x)\n",
    "            face_height = abs(bbox[2].y - bbox[0].y)\n",
    "            \n",
    "            # Ein Gesicht gilt als prominent, wenn es mindestens 15% der Bildbreite oder -höhe einnimmt\n",
    "            # Angenommene Bildgröße: 1280x720 (typisches YouTube-Thumbnail)\n",
    "            is_prominent = (face_width > 192) or (face_height > 108)  # 15% von 1280 bzw. 720\n",
    "            \n",
    "            if is_prominent:\n",
    "                prominent_faces.append(face)\n",
    "        \n",
    "        # Text-Analyse mit strengeren Kriterien\n",
    "        text_annotations = response.text_annotations\n",
    "        has_prominent_text = False\n",
    "        if text_annotations:\n",
    "            # Erste Annotation enthält den gesamten Text und Boundingbox\n",
    "            main_text = text_annotations[0]\n",
    "            if main_text.description:\n",
    "                # Berechne die ungefähre Textgröße\n",
    "                vertices = main_text.bounding_poly.vertices\n",
    "                text_width = abs(vertices[1].x - vertices[0].x)\n",
    "                text_height = abs(vertices[2].y - vertices[0].y)\n",
    "                text_area = text_width * text_height\n",
    "                \n",
    "                # Text muss mindestens 10% des Bildes einnehmen\n",
    "                has_prominent_text = text_area > (800 * 600 * 0.1)  # Annahme eines 800x600 Thumbnails\n",
    "        \n",
    "        # Screenshot-Erkennung\n",
    "        labels = [label.description.lower() for label in response.label_annotations]\n",
    "        screenshot_indicators = ['screenshot', 'screen capture', 'user interface', 'display']\n",
    "        is_screenshot = any(indicator in labels for indicator in screenshot_indicators)\n",
    "        \n",
    "        # Kategorisierung mit strengeren Kriterien\n",
    "        if len(prominent_faces) > 0 and len(prominent_faces) <= 2:\n",
    "            category = ThumbnailCategory.PERSON_FOCUSED\n",
    "            confidence = 0.9 if len(prominent_faces) > 0 else 0.5\n",
    "        elif is_screenshot and has_prominent_text:\n",
    "            category = ThumbnailCategory.SCREENSHOT_WITH_TEXT\n",
    "            confidence = 0.7\n",
    "        else:\n",
    "            category = ThumbnailCategory.SCREENSHOT_ONLY\n",
    "            confidence = 0.8 if is_screenshot else 0.5\n",
    "            \n",
    "        return {\n",
    "            'video_id': os.path.basename(image_path).replace('.jpg', ''),\n",
    "            'category': category.name,\n",
    "            'category_score': category.value,\n",
    "            'confidence': confidence,\n",
    "            'has_prominent_text': has_prominent_text,\n",
    "            'prominent_face_count': len(prominent_faces),\n",
    "            'is_screenshot': is_screenshot,\n",
    "            'thumbnail_path': image_path  # Pfad für spätere Visualisierung\n",
    "        }\n",
    "\n",
    "    def process_sample_videos(self, video_id_column: str = 'video_id'):\n",
    "        \"\"\"Verarbeitet eine Stichprobe von Videos\"\"\"\n",
    "        # CSV laden mit korrekten Parametern\n",
    "        print(f\"Lade CSV von {CSV_PATH}...\")\n",
    "        df = pd.read_csv(\n",
    "            CSV_PATH,\n",
    "            sep=';',              # Semikolon als Trennzeichen\n",
    "            quotechar='\"',        # Anführungszeichen für Texte\n",
    "            encoding='utf-8',     # Zeichenkodierung\n",
    "            engine='python'       # Robusterer Python-Parser\n",
    "        )\n",
    "        print(f\"Verfügbare Spalten: {', '.join(df.columns)}\")\n",
    "        \n",
    "        if video_id_column not in df.columns:\n",
    "            raise ValueError(f\"Spalte '{video_id_column}' nicht in der CSV gefunden!\")\n",
    "        \n",
    "        # Zufällige Stichprobe von Video-IDs\n",
    "        video_ids = df[video_id_column].dropna().unique()\n",
    "        sample_ids = np.random.choice(video_ids, size=min(SAMPLE_SIZE, len(video_ids)), replace=False)\n",
    "        print(f\"Ausgewählt: {len(sample_ids)} zufällige Video-IDs\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Thumbnails herunterladen und analysieren\n",
    "        for video_id in sample_ids:\n",
    "            print(f\"\\nVerarbeite Video {video_id}...\")\n",
    "            \n",
    "            thumbnail_path = self.download_thumbnail(video_id)\n",
    "            if thumbnail_path:\n",
    "                try:\n",
    "                    analysis = self.analyze_image(thumbnail_path)\n",
    "                    results.append(analysis)\n",
    "                except Exception as e:\n",
    "                    print(f\"Fehler bei der Analyse von {video_id}: {str(e)}\")\n",
    "        \n",
    "        # Ergebnisse in DataFrame konvertieren und speichern\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df = results_df.sort_values('category_score', ascending=False)\n",
    "        results_df.to_csv(ANALYSIS_OUTPUT, index=False)\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "def generate_html_report(results_df: pd.DataFrame) -> str:\n",
    "    \"\"\"Generiert einen HTML-Bericht mit Thumbnails und Analyseergebnissen\"\"\"\n",
    "    html = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <style>\n",
    "            .container { max-width: 1200px; margin: 0 auto; padding: 20px; }\n",
    "            .thumbnail { margin: 20px 0; padding: 20px; border: 1px solid #ddd; }\n",
    "            .thumbnail img { max-width: 400px; height: auto; }\n",
    "            .category-1 { border-left: 5px solid #ff4444; }\n",
    "            .category-2 { border-left: 5px solid #ffbb33; }\n",
    "            .category-3 { border-left: 5px solid #00C851; }\n",
    "            .details { margin-top: 10px; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"container\">\n",
    "            <h1>Thumbnail Analyse - Stichprobenergebnisse</h1>\n",
    "    \"\"\"\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        category_class = f\"category-{row['category_score']}\"\n",
    "        html += f\"\"\"\n",
    "            <div class=\"thumbnail {category_class}\">\n",
    "                <img src=\"{row['thumbnail_path']}\" alt=\"Thumbnail {row['video_id']}\">\n",
    "                <div class=\"details\">\n",
    "                    <h3>Video ID: {row['video_id']}</h3>\n",
    "                    <p>Kategorie: {row['category']} (Score: {row['category_score']})</p>\n",
    "                    <p>Prägnanter Text: {'Ja' if row['has_prominent_text'] else 'Nein'}</p>\n",
    "                    <p>Prägnante Personen: {row['prominent_face_count']}</p>\n",
    "                    <p>Screenshot erkannt: {'Ja' if row['is_screenshot'] else 'Nein'}</p>\n",
    "                    <p>Konfidenz: {row['confidence']:.2f}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # HTML-Bericht speichern\n",
    "    with open('thumbnail_analysis_report.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(html)\n",
    "    \n",
    "    return 'thumbnail_analysis_report.html'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        processor = YouTubeThumbnailProcessor()\n",
    "        results = processor.process_sample_videos()\n",
    "        \n",
    "        # HTML-Bericht generieren\n",
    "        report_path = generate_html_report(results)\n",
    "        print(f\"\\nAnalyse abgeschlossen. Bericht wurde erstellt: {report_path}\")\n",
    "        print(\"\\nÜbersicht der Kategorien:\")\n",
    "        print(results['category'].value_counts())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ein Fehler ist aufgetreten: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Code analysiert YouTube-Video-Thumbnails mithilfe der Google Cloud Vision API\n",
    "Hauptfunktionen:\n",
    "\n",
    "Download von Thumbnails für bestimmte Video-IDs\n",
    "Analyse der Bilder auf zwei Hauptmerkmale:\n",
    "\n",
    "Präsenz von Gesichtern (inkl. Größenberechnung)\n",
    "Vorhandensein von Text\n",
    "\n",
    "##Es gibt drei Bewertungsstufen (Scores) für Thumbnails:\n",
    "\n",
    "Basic (Score 1): Nur Bild ohne Text/Gesicht\n",
    "With Text (Score 2): Bild mit deutlichem Text\n",
    "Optimal (Score 3): Bild mit Text und prominentem Gesicht\n",
    "\n",
    "#Arbeitsablauf:\n",
    "Liest Video-IDs aus einer CSV-Datei\n",
    "Wählt zufällig eine Stichprobe (standardmäßig 100 Videos)\n",
    "Lädt die Thumbnails herunter\n",
    "Analysiert jedes Thumbnail\n",
    "Speichert Zwischenergebnisse\n",
    "Erstellt einen HTML-Bericht mit visueller Übersicht und Statistiken\n",
    "\n",
    "#Der Code speichert seine Ergebnisse in:\n",
    "Eine Interim-CSV für Zwischenergebnisse\n",
    "Eine finale Analyse-CSV\n",
    "Einen HTML-Report zur visuellen Überprüfung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from google.cloud import vision\n",
    "from typing import Dict, List, Optional\n",
    "from enum import Enum\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "\n",
    "# Konstanten\n",
    "CREDENTIALS_PATH = r\"C:\\Users\\Laukat\\OneDrive - Mediengruppe RTL\\HDM Data Analyti\\Google cloud vision API\\youtube-analyse-444120-8ce0b527c6e8.json\"\n",
    "CSV_PATH = r\"C:\\Users\\Laukat\\OneDrive - Mediengruppe RTL\\HDM Data Analyti\\project\\data\\raw\\3_update_allewerte_Themenkategoristiert_dopplungen_bereinigt.csv\"\n",
    "OUTPUT_DIR = \"youtube_thumbnails\"\n",
    "ANALYSIS_OUTPUT = \"sample_analysis_results.csv\"\n",
    "INTERIM_OUTPUT = \"interim_analysis_results.csv\"\n",
    "HTML_REPORT = \"thumbnail_report.html\"\n",
    "\n",
    "class ThumbnailScore(Enum):\n",
    "    BASIC = 1        # Nur Screenshot/Bild ohne Text oder präsentes Gesicht\n",
    "    WITH_TEXT = 2    # Bild mit Titel in Textform\n",
    "    OPTIMAL = 3      # Präsentes Gesicht + Titel in Textform\n",
    "\n",
    "class YouTubeThumbnailProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialisiert den Processor mit Google Vision Client\"\"\"\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = CREDENTIALS_PATH\n",
    "        self.vision_client = vision.ImageAnnotatorClient()\n",
    "        \n",
    "        if not os.path.exists(OUTPUT_DIR):\n",
    "            os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    def download_thumbnail(self, video_id: str, quality: str = \"maxresdefault\") -> Optional[str]:\n",
    "        \"\"\"Lädt ein einzelnes Thumbnail herunter\"\"\"\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"{video_id}.jpg\")\n",
    "        \n",
    "        # Prüfe ob Thumbnail bereits existiert\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Thumbnail für {video_id} bereits vorhanden, überspringe Download\")\n",
    "            return output_path\n",
    "            \n",
    "        base_url = \"https://img.youtube.com/vi/\"\n",
    "        thumbnail_url = urljoin(base_url, f\"{video_id}/{quality}.jpg\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(thumbnail_url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(output_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "                \n",
    "            print(f\"Erfolgreich heruntergeladen: {video_id}\")\n",
    "            return output_path\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Fehler beim Herunterladen von {video_id}: {str(e)}\")\n",
    "            if quality == \"maxresdefault\":\n",
    "                print(\"Versuche mit niedrigerer Qualität (hqdefault)...\")\n",
    "                return self.download_thumbnail(video_id, \"hqdefault\")\n",
    "            return None\n",
    "\n",
    "    def analyze_image(self, image_path: str) -> dict:\n",
    "        \"\"\"Analysiert ein Thumbnail mit Google Vision API\"\"\"\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        \n",
    "        image = vision.Image(content=content)\n",
    "        \n",
    "        features = [\n",
    "            vision.Feature(type_=vision.Feature.Type.FACE_DETECTION),\n",
    "            vision.Feature(type_=vision.Feature.Type.TEXT_DETECTION)\n",
    "        ]\n",
    "        request = vision.AnnotateImageRequest(image=image, features=features)\n",
    "        response = self.vision_client.annotate_image(request=request)\n",
    "        \n",
    "        # Gesichtserkennung\n",
    "        faces = response.face_annotations\n",
    "        img_width = 1280  # Standard YouTube Thumbnail Breite\n",
    "        img_height = 720  # Standard YouTube Thumbnail Höhe\n",
    "        \n",
    "        has_prominent_face = False\n",
    "        for face in faces:\n",
    "            width = abs(face.bounding_poly.vertices[2].x - face.bounding_poly.vertices[0].x)\n",
    "            height = abs(face.bounding_poly.vertices[2].y - face.bounding_poly.vertices[0].y)\n",
    "            \n",
    "            face_area = width * height\n",
    "            total_area = img_width * img_height\n",
    "            area_percentage = (face_area / total_area) * 100\n",
    "            \n",
    "            print(f\"Gesichtsanalyse für {os.path.basename(image_path)}:\")\n",
    "            print(f\"- Breite: {width}px ({(width/img_width)*100:.1f}%)\")\n",
    "            print(f\"- Höhe: {height}px ({(height/img_height)*100:.1f}%)\")\n",
    "            print(f\"- Flächenanteil: {area_percentage:.1f}%\")\n",
    "            \n",
    "            # Kriterien für prominentes Gesicht\n",
    "            has_prominent_face = (\n",
    "                (width > img_width * 0.2 or height > img_height * 0.2) or\n",
    "                area_percentage > 10\n",
    "            )\n",
    "            if has_prominent_face:\n",
    "                break\n",
    "        \n",
    "        # Texterkennung\n",
    "        has_prominent_text = False\n",
    "        if response.text_annotations:\n",
    "            main_text = response.text_annotations[0]\n",
    "            text_vertices = main_text.bounding_poly.vertices\n",
    "            text_width = abs(text_vertices[1].x - text_vertices[0].x)\n",
    "            text_height = abs(text_vertices[2].y - text_vertices[0].y)\n",
    "            text_area = text_width * text_height\n",
    "            has_prominent_text = text_area > (img_width * img_height * 0.1)\n",
    "        \n",
    "        # Score bestimmen\n",
    "        if has_prominent_face and has_prominent_text:\n",
    "            score = ThumbnailScore.OPTIMAL\n",
    "        elif has_prominent_text:\n",
    "            score = ThumbnailScore.WITH_TEXT\n",
    "        else:\n",
    "            score = ThumbnailScore.BASIC\n",
    "            \n",
    "        return {\n",
    "            'video_id': os.path.basename(image_path).replace('.jpg', ''),\n",
    "            'score': score.value,\n",
    "            'thumbnail_path': image_path\n",
    "        }\n",
    "\n",
    "    def process_videos(self, sample_size: int = 100):\n",
    "        \"\"\"Verarbeitet eine Stichprobe von Videos\"\"\"\n",
    "        print(f\"Lade CSV von {CSV_PATH}...\")\n",
    "        df = pd.read_csv(CSV_PATH, \n",
    "                        sep=';',\n",
    "                        decimal=',',\n",
    "                        encoding='utf-8')\n",
    "        \n",
    "        # Zufällige Auswahl von Videos\n",
    "        video_ids = df['video_id'].dropna().unique()\n",
    "        selected_ids = np.random.choice(video_ids, size=min(sample_size, len(video_ids)), replace=False)\n",
    "        print(f\"Ausgewählt: {len(selected_ids)} zufällige Video-IDs\")\n",
    "        \n",
    "        results = []\n",
    "        for idx, video_id in enumerate(selected_ids, 1):\n",
    "            print(f\"\\nVerarbeite Video {idx}/{sample_size}: {video_id}\")\n",
    "            \n",
    "            thumbnail_path = self.download_thumbnail(video_id)\n",
    "            if thumbnail_path:\n",
    "                try:\n",
    "                    analysis = self.analyze_image(thumbnail_path)\n",
    "                    results.append(analysis)\n",
    "                    \n",
    "                    # Zwischenspeicherung der Ergebnisse\n",
    "                    temp_df = pd.DataFrame(results)\n",
    "                    temp_df.to_csv(INTERIM_OUTPUT, \n",
    "                                 sep=';',\n",
    "                                 decimal=',',\n",
    "                                 index=False,\n",
    "                                 encoding='utf-8')\n",
    "                    \n",
    "                    # Update der Stichproben-CSV\n",
    "                    sample_df = df[df['video_id'].isin(selected_ids)].copy()\n",
    "                    score_dict = dict(zip(temp_df['video_id'], temp_df['score']))\n",
    "                    sample_df['Gestaltung_Thumbnail'] = sample_df['video_id'].map(score_dict)\n",
    "                    sample_df.to_csv(ANALYSIS_OUTPUT, \n",
    "                                   sep=';',\n",
    "                                   decimal=',',\n",
    "                                   index=False,\n",
    "                                   encoding='utf-8')\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Fehler bei der Analyse von {video_id}: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "def generate_html_report(results_df: pd.DataFrame):\n",
    "    \"\"\"Generiert einen HTML-Bericht zur visuellen Überprüfung\"\"\"\n",
    "    score_counts = results_df['score'].value_counts()\n",
    "    \n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>Thumbnail Analyse</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "        .container {{ max-width: 1200px; margin: 0 auto; padding: 20px; }}\n",
    "        .stats {{ \n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(3, 1fr);\n",
    "            gap: 20px;\n",
    "            margin-bottom: 30px;\n",
    "        }}\n",
    "        .stat-box {{ \n",
    "            padding: 20px;\n",
    "            background: #f8f9fa;\n",
    "            border-radius: 8px;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        .thumbnails {{ \n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));\n",
    "            gap: 20px;\n",
    "        }}\n",
    "        .thumbnail {{ \n",
    "            padding: 15px;\n",
    "            background: white;\n",
    "            border-radius: 8px;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        .thumbnail img {{ \n",
    "            width: 100%;\n",
    "            height: auto;\n",
    "            border-radius: 4px;\n",
    "        }}\n",
    "        .score-1 {{ color: #dc3545; }}\n",
    "        .score-2 {{ color: #ffc107; }}\n",
    "        .score-3 {{ color: #28a745; }}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Thumbnail Analyse Ergebnisse</h1>\n",
    "        \n",
    "        <div class=\"stats\">\n",
    "            <div class=\"stat-box\">\n",
    "                <h3>Score 3 (Optimal)</h3>\n",
    "                <p>{score_counts.get(3, 0)} Videos</p>\n",
    "            </div>\n",
    "            <div class=\"stat-box\">\n",
    "                <h3>Score 2 (Mit Text)</h3>\n",
    "                <p>{score_counts.get(2, 0)} Videos</p>\n",
    "            </div>\n",
    "            <div class=\"stat-box\">\n",
    "                <h3>Score 1 (Basic)</h3>\n",
    "                <p>{score_counts.get(1, 0)} Videos</p>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"thumbnails\">\n",
    "    \"\"\"\n",
    "    \n",
    "    # Nach Score sortieren\n",
    "    sorted_results = results_df.sort_values('score', ascending=False)\n",
    "    \n",
    "    for _, row in sorted_results.iterrows():\n",
    "        html_content += f\"\"\"\n",
    "            <div class=\"thumbnail\">\n",
    "                <img src=\"{row['thumbnail_path']}\" alt=\"Thumbnail {row['video_id']}\">\n",
    "                <p>Video ID: {row['video_id']}</p>\n",
    "                <p class=\"score-{row['score']}\">Score: {row['score']}</p>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "        </div>\n",
    "    </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(HTML_REPORT, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    return HTML_REPORT\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        processor = YouTubeThumbnailProcessor()\n",
    "        results = processor.process_videos(sample_size=100)\n",
    "        \n",
    "        if not results.empty:\n",
    "            report_path = generate_html_report(results)\n",
    "            print(f\"\\nHTML-Bericht wurde erstellt: {report_path}\")\n",
    "            print(\"\\nVerteilung der Scores:\")\n",
    "            print(results['score'].value_counts().sort_index())\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProgramm wurde durch Benutzer unterbrochen.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ein Fehler ist aufgetreten: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachfolgender Code, um eine SEO-Bewertung der Titel über OpenAI zu schreiben\n",
    "Dateipfad und Spalten:\n",
    "Verwendet den exakten Pfad zu deiner CSV-Datei\n",
    "Liest aus der Spalte \"title\"\n",
    "Schreibt in die Spalte \"Bewertung_Titel\"\n",
    "Behält alle anderen Spalten unverändert bei\n",
    "Deutsches Format:\n",
    "Verwendet Semikolon als Trennzeichen\n",
    "Verwendet Komma als Dezimaltrennzeichen\n",
    "Behält diese Formatierung beim Speichern bei\n",
    "In-Place Speicherung:\n",
    "Überschreibt die originale Datei mit den neuen Bewertungen\n",
    "Behält dabei das Format und alle anderen Daten bei\n",
    "Verwendung:\n",
    "\n",
    "#Das Skript wird:\n",
    "\n",
    "Die Analyse automatisch in Batches durchführen\n",
    "Regelmäßig Checkpoints speichern\n",
    "Bei einem Abbruch an der letzten Stelle weitermachen\n",
    "Die Ergebnisse direkt in deine CSV-Datei schreiben\n",
    "Detaillierte Logs und Statistiken ausgeben\n",
    "Möchtest du das Skript so testen oder soll ich noch etwas anpassen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from typing import Optional, List, Dict\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "# Logging-Konfiguration\n",
    "log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=log_format,\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('seo_analysis.log', encoding='utf-8')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CheckpointManager:\n",
    "    def __init__(self, checkpoint_dir: str = \"checkpoints\"):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def _generate_session_id(self, csv_path: str) -> str:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "        file_hash = hashlib.md5(str(csv_path).encode()).hexdigest()[:8]\n",
    "        return f\"{timestamp}_{file_hash}\"\n",
    "        \n",
    "    def save_checkpoint(self, session_id: str, processed_data: Dict) -> None:\n",
    "        checkpoint_file = self.checkpoint_dir / f\"checkpoint_{session_id}.json\"\n",
    "        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_data, f, ensure_ascii=False, indent=2)\n",
    "        logger.info(f\"Checkpoint gespeichert: {checkpoint_file}\")\n",
    "            \n",
    "    def load_checkpoint(self, session_id: str) -> Optional[Dict]:\n",
    "        checkpoint_file = self.checkpoint_dir / f\"checkpoint_{session_id}.json\"\n",
    "        if checkpoint_file.exists():\n",
    "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "\n",
    "class YouTubeSEOScorer:\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.checkpoint_manager = CheckpointManager()\n",
    "        \n",
    "        self.system_prompt = \"\"\"Du bist ein YouTube SEO Experte, spezialisiert auf Nachrichtenkanäle. \n",
    "        Bewerte den YouTube-Titel nach folgenden Kriterien und vergib eine Gesamtpunktzahl von 0-10. \n",
    "        WICHTIG: Gib NUR die Gesamtpunktzahl als einzelne Zahl zurück, OHNE Text oder Erklärungen.\n",
    "        Beispiele korrekter Antworten:\n",
    "        7,5\n",
    "        8,0\n",
    "        6,5\n",
    "        \n",
    "        Bewerte nach diesen Kriterien:\n",
    "\n",
    "        1. Titel-Optimierung (max. 2 Punkte)\n",
    "        - Länge zwischen 40-70 Zeichen (+0.5)\n",
    "        - Hauptkeyword in den ersten 3-4 Wörtern (+0.5)\n",
    "        - Klare Struktur mit natürlicher Lesefluss (+0.5)\n",
    "        - Verwendung von Trennzeichen (|, -, :) wo sinnvoll (+0.5)\n",
    "\n",
    "        2. Nachrichtenwert & Aktualität (max. 2 Punkte)\n",
    "        - Klare Vermittlung der Nachrichtenrelevanz (+0.5)\n",
    "        - Zeitliche Einordnung wenn relevant (+0.5)\n",
    "        - Balance zwischen Aktualität und Evergreen-Potenzial (+0.5)\n",
    "        - Korrekte Priorisierung der Information (+0.5)\n",
    "\n",
    "        3. Keyword-Optimierung (max. 2 Punkte)\n",
    "        - Verwendung relevanter Nachrichtenkeywords (+0.5)\n",
    "        - Natürliche Integration von Suchbegriffen (+0.5)\n",
    "        - LSI Keywords / thematisch verwandte Begriffe (+0.5)\n",
    "        - Vermeidung von Keyword-Stuffing (+0.5)\n",
    "\n",
    "        4. Zielgruppen-Ansprache (max. 2 Punkte)\n",
    "        - Verständliche Sprache für Nachrichtenpublikum (+0.5)\n",
    "        - Seriosität und Professionalität (+0.5)\n",
    "        - Klare Themenankündigung (+0.5)\n",
    "        - Zielgruppengerechte Formulierung (+0.5)\n",
    "\n",
    "        5. Technische Optimierung (max. 2 Punkte)\n",
    "        - Keine übermäßige Großschreibung (+0.5)\n",
    "        - Korrekte Zeichensetzung (+0.5)\n",
    "        - Keine Spam-Taktiken oder Clickbait (+0.5)\n",
    "        - Mobile-freundliche Länge & Format (+0.5)\n",
    "\n",
    "        WICHTIG - FORMAT DER ANTWORT:\n",
    "        - Gib ausschließlich eine einzelne Dezimalzahl zurück\n",
    "        - Verwende ein Komma als Dezimaltrennzeichen\n",
    "        - Keine Erklärungen oder zusätzlicher Text\n",
    "        - Keine Aufschlüsselung der Einzelkriterien\n",
    "        \n",
    "        Beispiele korrekter Antworten:\n",
    "        7,5\n",
    "        8,0\n",
    "        6,5\"\"\"\n",
    "\n",
    "    async def analyze_title(self, title: str, retry_count: int = 3) -> Optional[float]:\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": f\"Bewerte diesen Titel: {title}\"}\n",
    "                    ],\n",
    "                    temperature=0.3\n",
    "                )\n",
    "                \n",
    "                score = float(response.choices[0].message.content.strip().replace(',', '.'))\n",
    "                return round(score, 1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < retry_count - 1:\n",
    "                    wait_time = (attempt + 1) * 2\n",
    "                    logger.warning(f\"Fehler bei Titel '{title}'. Retry in {wait_time}s... ({str(e)})\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                else:\n",
    "                    logger.error(f\"Endgültiger Fehler bei '{title}' nach {retry_count} Versuchen: {str(e)}\")\n",
    "                    return None\n",
    "\n",
    "    async def analyze_batch(self, titles: List[str], session_id: str, start_idx: int = 0, batch_size: int = 10) -> Dict[int, float]:\n",
    "        results = {}\n",
    "        \n",
    "        for i in range(start_idx, len(titles), batch_size):\n",
    "            batch = titles[i:i + batch_size]\n",
    "            tasks = [self.analyze_title(title) for title in batch]\n",
    "            \n",
    "            try:\n",
    "                batch_scores = await asyncio.gather(*tasks)\n",
    "                \n",
    "                # Ergebnisse mit Index speichern\n",
    "                for idx, score in enumerate(batch_scores, start=i):\n",
    "                    if score is not None:\n",
    "                        results[idx] = score\n",
    "                \n",
    "                # Checkpoint\n",
    "                checkpoint_data = {\n",
    "                    'last_processed_index': i + batch_size,\n",
    "                    'results': results\n",
    "                }\n",
    "                self.checkpoint_manager.save_checkpoint(session_id, checkpoint_data)\n",
    "                \n",
    "                # Fortschritt\n",
    "                processed = min(i + batch_size, len(titles))\n",
    "                logger.info(f\"Fortschritt: {processed}/{len(titles)} Titel ({processed/len(titles)*100:.1f}%)\")\n",
    "                \n",
    "                if i + batch_size < len(titles):\n",
    "                    await asyncio.sleep(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Fehler beim Batch-Processing (Index {i}): {str(e)}\")\n",
    "                checkpoint_data = {\n",
    "                    'last_processed_index': i,\n",
    "                    'results': results\n",
    "                }\n",
    "                self.checkpoint_manager.save_checkpoint(session_id, checkpoint_data)\n",
    "                \n",
    "        return results\n",
    "\n",
    "    async def analyze_from_csv(self, csv_path: str) -> None:\n",
    "        try:\n",
    "            csv_path = Path(csv_path)\n",
    "            if not csv_path.exists():\n",
    "                raise FileNotFoundError(f\"CSV-Datei nicht gefunden: {csv_path}\")\n",
    "            \n",
    "            # Session ID für Checkpointing\n",
    "            session_id = self.checkpoint_manager._generate_session_id(str(csv_path))\n",
    "            \n",
    "            # CSV mit deutschem Format laden\n",
    "            df = pd.read_csv(csv_path, sep=';', decimal=',', encoding='utf-8')\n",
    "            \n",
    "            if 'title' not in df.columns:\n",
    "                raise ValueError(\"Spalte 'title' nicht gefunden!\")\n",
    "            \n",
    "            # Checkpoint prüfen\n",
    "            checkpoint = self.checkpoint_manager.load_checkpoint(session_id)\n",
    "            start_idx = 0\n",
    "            results = {}\n",
    "            \n",
    "            if checkpoint:\n",
    "                start_idx = checkpoint['last_processed_index']\n",
    "                results = {int(k): v for k, v in checkpoint['results'].items()}\n",
    "                logger.info(f\"Checkpoint gefunden. Fortsetzen ab Index {start_idx}\")\n",
    "            \n",
    "            # Startzeit\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Noch nicht verarbeitete Titel analysieren\n",
    "            titles = df['title'].tolist()\n",
    "            if start_idx < len(titles):\n",
    "                new_results = await self.analyze_batch(\n",
    "                    titles=titles,\n",
    "                    session_id=session_id,\n",
    "                    start_idx=start_idx,\n",
    "                    batch_size=10\n",
    "                )\n",
    "                results.update(new_results)\n",
    "            \n",
    "            # Ergebnisse in DataFrame integrieren\n",
    "            df['Bewertung_Titel'] = pd.Series(results)\n",
    "            \n",
    "            # Ergebnisse im deutschen Format speichern\n",
    "            df.to_csv(csv_path, sep=';', decimal=',', index=False, encoding='utf-8')\n",
    "            \n",
    "            # Statistiken\n",
    "            scores = list(results.values())\n",
    "            stats = {\n",
    "                'Verarbeitete Titel': len(scores),\n",
    "                'Durchschnittlicher Score': sum(scores) / len(scores) if scores else 0,\n",
    "                'Titel unter 5 Punkten': sum(1 for s in scores if s < 5),\n",
    "                'Titel über 8 Punkten': sum(1 for s in scores if s >= 8),\n",
    "                'Verarbeitungszeit': str(datetime.now() - start_time)\n",
    "            }\n",
    "            \n",
    "            logger.info(\"\\nAnalyse abgeschlossen:\")\n",
    "            for key, value in stats.items():\n",
    "                logger.info(f\"{key}: {value}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Kritischer Fehler: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausführung der Aktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# API Key\n",
    "API_KEY = 'dein api code'  # Dein API Key hier\n",
    "\n",
    "# Pfad zur CSV\n",
    "CSV_PATH = r\"C:\\Users\\laukat\\OneDrive - Mediengruppe RTL\\HDM Data Analyti\\project\\data\\raw\\4_update_allewerte_Themenkategoristiert_dopplungen_bereinigt_thumbnailScores.csv\"\n",
    "\n",
    "# Analyzer initialisieren und ausführen\n",
    "async def main():\n",
    "    analyzer = YouTubeSEOScorer(\n",
    "        api_key=API_KEY,\n",
    "        model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "    await analyzer.analyze_from_csv(CSV_PATH)\n",
    "\n",
    "# Starten der Analyse\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code, falls es Zeilen ohne Bewwrtung gibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# API Key\n",
    "API_KEY = 'dein api code'  # Dein API Key hier\n",
    "\n",
    "# Pfad zur CSV\n",
    "CSV_PATH = r\"C:\\Users\\laukat\\OneDrive - Mediengruppe RTL\\HDM Data Analyti\\project\\data\\raw\\4_update_allewerte_Themenkategoristiert_dopplungen_bereinigt_thumbnailScores.csv\"\n",
    "\n",
    "async def analyze_missing_ratings():\n",
    "    # Daten laden\n",
    "    df = pd.read_csv(CSV_PATH, sep=';', decimal=',', encoding='utf-8')\n",
    "    \n",
    "    # Fehlende Bewertungen identifizieren\n",
    "    missing_ratings = df[df['Bewertung_Titel'].isna()]\n",
    "    print(f\"Gefunden: {len(missing_ratings)} Titel ohne Bewertung\")\n",
    "    \n",
    "    if len(missing_ratings) == 0:\n",
    "        print(\"Keine fehlenden Bewertungen gefunden!\")\n",
    "        return\n",
    "    \n",
    "    # Analyzer initialisieren\n",
    "    analyzer = YouTubeSEOScorer(\n",
    "        api_key=API_KEY,\n",
    "        model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "    \n",
    "    # Analyse der fehlenden Bewertungen\n",
    "    results = {}\n",
    "    for idx in missing_ratings.index:\n",
    "        title = df.at[idx, 'title']\n",
    "        try:\n",
    "            score = await analyzer.analyze_title(title)\n",
    "            if score is not None:\n",
    "                results[idx] = score\n",
    "                print(f\"Zeile {idx}: '{title[:70]}...' - Score: {score}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei Zeile {idx}: {str(e)}\")\n",
    "        \n",
    "        # Kurze Pause zwischen den Anfragen\n",
    "        await asyncio.sleep(0.5)\n",
    "    \n",
    "    # Ergebnisse in DataFrame speichern\n",
    "    for idx, score in results.items():\n",
    "        df.at[idx, 'Bewertung_Titel'] = score\n",
    "    \n",
    "    # Speichern mit deutschem Format\n",
    "    df.to_csv(CSV_PATH, sep=';', decimal=',', index=False, encoding='utf-8')\n",
    "    \n",
    "    print(\"\\nAnalyse der fehlenden Bewertungen abgeschlossen!\")\n",
    "    print(f\"Anzahl erfolgreich bewerteter Titel: {len(results)}\")\n",
    "    print(f\"Verbleibende fehlende Bewertungen: {len(missing_ratings) - len(results)}\")\n",
    "\n",
    "# Ausführung\n",
    "await analyze_missing_ratings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

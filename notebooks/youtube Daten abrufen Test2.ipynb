{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install isodate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code zum Abrufen fast aller Daten des youtube Channels. Es müssen noch die API-Daten und der Link zur json-Datei eingetragen werden. Außerdem der Datumsbereich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Konfiguration - Hier Ihre Werte eintragen\n",
    "CHANNEL_ID = \"UCSeil5V81-mEGB1-VNR7YEA\"  # Ihre YouTube Channel ID\n",
    "DATA_API_V3_KEY = \"\"  # API Key für YouTube Data API v3\n",
    "#ANALYTICS_API_KEY = \"\"  # API Key für YouTube Analytics API\n",
    "CREDENTIALS_PATH = \"\"\n",
    "\n",
    "\n",
    "# Datum Konfiguration - Format: \"DD.MM.YYYY\"\n",
    "START_DATE = \"01.01.2024\"  # z.B. \"01.09.2024\"\n",
    "END_DATE = \"31.01.2024\"   # z.B. \"02.09.2024\"\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import isodate\n",
    "\n",
    "# OAuth 2.0 Scopes\n",
    "SCOPES = ['https://www.googleapis.com/auth/yt-analytics.readonly']\n",
    "\n",
    "def get_video_data_from_api():\n",
    "    \"\"\"YouTube Data API v3 Abfrage für Basis-Videodaten\"\"\"\n",
    "    youtube = build('youtube', 'v3', developerKey=DATA_API_V3_KEY)\n",
    "    \n",
    "    start_date = datetime.strptime(START_DATE, '%d.%m.%Y')\n",
    "    end_date = datetime.strptime(END_DATE, '%d.%m.%Y').replace(hour=23, minute=59, second=59)\n",
    "    \n",
    "    start_date_rfc = start_date.isoformat() + 'Z'\n",
    "    end_date_rfc = end_date.isoformat() + 'Z'\n",
    "    \n",
    "    videos_data = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    while True:\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            channelId=CHANNEL_ID,\n",
    "            maxResults=50,\n",
    "            order='date',\n",
    "            publishedAfter=start_date_rfc,\n",
    "            publishedBefore=end_date_rfc,\n",
    "            type='video',\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        \n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response['items']:\n",
    "            video_id = item['id']['videoId']\n",
    "            \n",
    "            video_request = youtube.videos().list(\n",
    "                part='snippet,contentDetails,status',\n",
    "                id=video_id\n",
    "            )\n",
    "            video_response = video_request.execute()\n",
    "            \n",
    "            if video_response['items']:\n",
    "                video_info = video_response['items'][0]\n",
    "                duration = int(isodate.parse_duration(video_info['contentDetails']['duration']).total_seconds())\n",
    "                \n",
    "                video_data = {\n",
    "                    'video_id': video_id,\n",
    "                    'title': video_info['snippet']['title'],\n",
    "                    'publish_date': video_info['snippet']['publishedAt'],\n",
    "                    'video_length_seconds': duration,\n",
    "                    'privacy_status': video_info['status']['privacyStatus']\n",
    "                }\n",
    "                videos_data.append(video_data)\n",
    "        \n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(videos_data)\n",
    "\n",
    "def get_analytics_data(analytics, video_id, start_date, end_date):\n",
    "    \"\"\"YouTube Analytics API Abfrage für detaillierte Metriken\"\"\"\n",
    "    try:\n",
    "        # Performance Metriken\n",
    "        metrics = [\n",
    "            'estimatedMinutesWatched',\n",
    "            'averageViewDuration',\n",
    "            'averageViewPercentage',\n",
    "            'subscribersGained',\n",
    "            'subscribersLost',\n",
    "            'views',\n",
    "            'likes',\n",
    "            'dislikes',\n",
    "            'shares',\n",
    "            'comments'\n",
    "        ]\n",
    "        \n",
    "        perf_response = analytics.reports().query(\n",
    "            ids=f'channel=={CHANNEL_ID}',\n",
    "            startDate=start_date.strftime('%Y-%m-%d'),\n",
    "            endDate=end_date.strftime('%Y-%m-%d'),\n",
    "            metrics=','.join(metrics),\n",
    "            filters=f'video=={video_id}'\n",
    "        ).execute()\n",
    "\n",
    "        # Demografische Daten\n",
    "        demo_response = analytics.reports().query(\n",
    "            ids=f'channel=={CHANNEL_ID}',\n",
    "            startDate=start_date.strftime('%Y-%m-%d'),\n",
    "            endDate=end_date.strftime('%Y-%m-%d'),\n",
    "            metrics='viewerPercentage',\n",
    "            dimensions='ageGroup,gender',\n",
    "            filters=f'video=={video_id}'\n",
    "        ).execute()\n",
    "\n",
    "        # Basis-Daten sammeln\n",
    "        data = {\n",
    "            'video_id': video_id,\n",
    "            'wiedergabezeit_minuten': 0,\n",
    "            'durchschnittliche_wiedergabedauer': 0,\n",
    "            'durchschnittliche_wiedergabedauer_prozent': 0,\n",
    "            'gewonnene_abonnenten': 0,\n",
    "            'verlorene_abonnenten': 0,\n",
    "            'aufrufe': 0,\n",
    "            'likes': 0,\n",
    "            'dislikes': 0,\n",
    "            'geteilte_inhalte': 0,\n",
    "            'kommentare': 0\n",
    "        }\n",
    "\n",
    "        # Performance Daten einfügen\n",
    "        if 'rows' in perf_response and perf_response['rows']:\n",
    "            row = perf_response['rows'][0]\n",
    "            data.update({\n",
    "                'wiedergabezeit_minuten': row[0],\n",
    "                'durchschnittliche_wiedergabedauer': row[1],\n",
    "                'durchschnittliche_wiedergabedauer_prozent': row[2],\n",
    "                'gewonnene_abonnenten': row[3],\n",
    "                'verlorene_abonnenten': row[4],\n",
    "                'aufrufe': row[5],\n",
    "                'likes': row[6],\n",
    "                'dislikes': row[7],\n",
    "                'geteilte_inhalte': row[8],\n",
    "                'kommentare': row[9]\n",
    "            })\n",
    "\n",
    "        # Demografische Daten einfügen\n",
    "        if 'rows' in demo_response:\n",
    "            for row in demo_response['rows']:\n",
    "                age_group, gender, percentage = row\n",
    "                key = f\"audience_{gender.lower()}_{age_group}\"\n",
    "                data[key] = percentage\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei Video {video_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"1. Hole Basis-Videodaten über YouTube Data API v3...\")\n",
    "        video_df = get_video_data_from_api()\n",
    "        print(f\"   - {len(video_df)} Videos gefunden\")\n",
    "\n",
    "        print(\"\\n2. Hole Analytics-Daten über YouTube Analytics API...\")\n",
    "        # Authentifizierung für Analytics API\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)\n",
    "        credentials = flow.run_local_server(port=0)\n",
    "        analytics = build('youtubeAnalytics', 'v2', credentials=credentials)\n",
    "        \n",
    "        start_date = datetime.strptime(START_DATE, '%d.%m.%Y')\n",
    "        end_date = datetime.strptime(END_DATE, '%d.%m.%Y')\n",
    "        \n",
    "        # Sammle Analytics-Daten für jedes Video\n",
    "        analytics_data = []\n",
    "        for index, row in video_df.iterrows():\n",
    "            print(f\"   - Verarbeite Video {index + 1} von {len(video_df)}: {row['video_id']}\")\n",
    "            data = get_analytics_data(analytics, row['video_id'], start_date, end_date)\n",
    "            if data:\n",
    "                analytics_data.append(data)\n",
    "\n",
    "        # Erstelle DataFrame aus Analytics-Daten\n",
    "        analytics_df = pd.DataFrame(analytics_data)\n",
    "        \n",
    "        # Merge beide DataFrames\n",
    "        final_df = pd.merge(video_df, analytics_df, on='video_id', how='left')\n",
    "        \n",
    "        # Speichere Ergebnis und stelle sicher, dass Videolänge als Ganzzahl gespeichtert wird\n",
    "        video_df['video_length_seconds'] = video_df['video_length_seconds'].astype(int)\n",
    "        output_filename = f'youtube_complete_analysis_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "        \n",
    "        print(f\"\\nAnalyse abgeschlossen!\")\n",
    "        print(f\"Daten wurden in {output_filename} gespeichert\")\n",
    "        print(f\"Anzahl der Videos: {len(final_df)}\")\n",
    "        print(f\"Anzahl der Spalten: {len(final_df.columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ein Fehler ist aufgetreten: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird eine Liste von video_ids abgefragt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Konfiguration - Hier Ihre Werte eintragen\n",
    "CHANNEL_ID = \"UCSeil5V81-mEGB1-VNR7YEA\"  # Ihre YouTube Channel ID\n",
    "DATA_API_V3_KEY = \"\"  # API Key für YouTube Data API v3\n",
    "#ANALYTICS_API_KEY = \"AIzaSyCBw3tVk-TnYLCZW2vmojQlYFBArzudU-A\"  # API Key für YouTube Analytics API\n",
    "CREDENTIALS_PATH = \"C:\\\\Users\\\\laukat\\\\OneDrive - Mediengruppe RTL\\\\HDM Data Analyti\\\\oauth 2.0\\\\client_secret_796311161257-bnk32mvms5t9fsfma7agbgrlt5fo54gi.apps.googleusercontent.com.json\"  # Pfad zur client_secrets.json\"\n",
    "\n",
    "\n",
    "# Datum Konfiguration - Format: \"DD.MM.YYYY\"\n",
    "START_DATE = \"01.01.2000\"\n",
    "END_DATE = \"31.01.2100\"\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import isodate\n",
    "\n",
    "# OAuth 2.0 Scopes\n",
    "SCOPES = ['https://www.googleapis.com/auth/yt-analytics.readonly']\n",
    "\n",
    "def get_video_data_from_list(video_ids):\n",
    "    \"\"\"Hole Videodaten basierend auf einer Liste von Video-IDs\"\"\"\n",
    "    youtube = build('youtube', 'v3', developerKey=DATA_API_V3_KEY)\n",
    "    \n",
    "    videos_data = []\n",
    "    for video_id in video_ids:\n",
    "        video_request = youtube.videos().list(\n",
    "            part='snippet,contentDetails,status',\n",
    "            id=video_id\n",
    "        )\n",
    "        video_response = video_request.execute()\n",
    "        \n",
    "        if video_response['items']:\n",
    "            video_info = video_response['items'][0]\n",
    "            duration = int(isodate.parse_duration(video_info['contentDetails']['duration']).total_seconds())\n",
    "            \n",
    "            video_data = {\n",
    "                'video_id': video_id,\n",
    "                'title': video_info['snippet']['title'],\n",
    "                'publish_date': video_info['snippet']['publishedAt'],\n",
    "                'video_length_seconds': duration,\n",
    "                'privacy_status': video_info['status']['privacyStatus']\n",
    "            }\n",
    "            videos_data.append(video_data)\n",
    "    \n",
    "    return pd.DataFrame(videos_data)\n",
    "\n",
    "def get_analytics_data(analytics, video_id, start_date, end_date):\n",
    "    \"\"\"YouTube Analytics API Abfrage für detaillierte Metriken\"\"\"\n",
    "    # Unverändert vom Original\n",
    "    ...\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"1. Lade Video-IDs aus der bereinigten Datei...\")\n",
    "        cleaned_video_df = pd.read_csv(\"cleaned_datenpaket2.csv\")\n",
    "        video_ids = cleaned_video_df['video_id'].tolist()\n",
    "        print(f\"   - {len(video_ids)} Video-IDs geladen\")\n",
    "        \n",
    "        print(\"\\n2. Hole Basis-Videodaten über YouTube Data API v3...\")\n",
    "        video_df = get_video_data_from_list(video_ids)\n",
    "        print(f\"   - {len(video_df)} Videodaten abgerufen\")\n",
    "\n",
    "        print(\"\\n3. Hole Analytics-Daten über YouTube Analytics API...\")\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)\n",
    "        credentials = flow.run_local_server(port=0)\n",
    "        analytics = build('youtubeAnalytics', 'v2', credentials=credentials)\n",
    "        \n",
    "        # Start- und Enddatum festlegen\n",
    "        start_date = datetime.strptime(START_DATE, '%d.%m.%Y')\n",
    "        end_date = datetime.strptime(END_DATE, '%d.%m.%Y')\n",
    "\n",
    "        # Analytics-Daten sammeln\n",
    "        analytics_data = []\n",
    "        for index, row in video_df.iterrows():\n",
    "            print(f\"   - Verarbeite Video {index + 1} von {len(video_df)}: {row['video_id']}\")\n",
    "            data = get_analytics_data(analytics, row['video_id'], start_date, end_date)\n",
    "            if data:\n",
    "                analytics_data.append(data)\n",
    "\n",
    "        analytics_df = pd.DataFrame(analytics_data)\n",
    "        \n",
    "        # Merge original cleaned file with API results\n",
    "        final_df = pd.merge(cleaned_video_df, video_df, on='video_id', how='left')  # Basisdaten\n",
    "        final_df = pd.merge(final_df, analytics_df, on='video_id', how='left')     # Analytics-Daten\n",
    "\n",
    "        output_filename = f'youtube_analysis_with_cleaned_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "\n",
    "        print(f\"\\nAnalyse abgeschlossen!\")\n",
    "        print(f\"Daten wurden in {output_filename} gespeichert\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ein Fehler ist aufgetreten: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Folgende Code ruft bestimmte video_ids ab, die in einer cs gespeichert sind.\n",
    "Der Code ist so eingerichtet, dass er:\n",
    "\n",
    "Kontinuierlich in die CSV-Datei speichert\n",
    "Einen Checkpoint für den letzten verarbeiteten Index anlegt\n",
    "Fortschrittsinformationen ausgibt.\n",
    "Der Code kann unterbrochen werden und arbeitet bei Wiederaufnahme an der letzten Stelle fort. \n",
    "Weietre Merkmale:\n",
    "Explizite Float-Konvertierung der Prozentwerte\n",
    "Mehr Debug-Ausgaben um zu sehen, was genau passiert\n",
    "Verhinderung der String-Formatierung beim Speichern\n",
    "Verifikation der gespeicherten Daten\n",
    "Verwendung von Komma als Dezimaltrennzeichen\n",
    "Semikolon als Spaltentrenner\n",
    "Formatierung aller numerischen Werte auf 2 Nachkommastellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Video-IDs aus der bereinigten Datei...\n",
      "Versuche Encoding: utf-8\n",
      "Versuche Encoding: iso-8859-1\n",
      "Geladen: 2536 Videos\n",
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=796311161257-bnk32mvms5t9fsfma7agbgrlt5fo54gi.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A53354%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fyt-analytics.readonly&state=xbHFXVhn42aEFxkQN1zr67A19k8clM&access_type=offline\n",
      "Starte Verarbeitung ab Index 2536\n",
      "\n",
      "Verarbeitung abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# API Konfiguration - Hier Ihre Werte eintragen\n",
    "CHANNEL_ID = \"UCSeil5V81-mEGB1-VNR7YEA\"  # Ihre YouTube Channel ID\n",
    "DATA_API_V3_KEY = \"AIzaSyCBw3tVk-TnYLCZW2vmojQlYFBArzudU-A\"  # API Key für YouTube Data API v3\n",
    "CREDENTIALS_PATH = \"C:\\\\Users\\\\laukat\\\\OneDrive - Mediengruppe RTL\\\\HDM Data Analyti\\\\oauth 2.0\\\\client_secret_796311161257-bnk32mvms5t9fsfma7agbgrlt5fo54gi.apps.googleusercontent.com.json\"\n",
    "\n",
    "# Datum Konfiguration\n",
    "START_DATE = \"01.01.2000\"\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# OAuth 2.0 Scopes\n",
    "SCOPES = ['https://www.googleapis.com/auth/yt-analytics.readonly']\n",
    "\n",
    "# Konstanten\n",
    "CHECKPOINT_FILE = 'analytics_checkpoint.json'\n",
    "OUTPUT_FILE = 'youtube_demographic_analysis_continuous.csv'\n",
    "\n",
    "def read_csv_with_encoding(file_path):\n",
    "    \"\"\"Versucht die CSV-Datei mit verschiedenen Encodings zu lesen\"\"\"\n",
    "    encodings = ['utf-8', 'iso-8859-1', 'cp1252', 'latin1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"Versuche Encoding: {encoding}\")\n",
    "            return pd.read_csv(file_path, encoding=encoding, sep=';')\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Lesen der Datei mit {encoding}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    raise Exception(\"Konnte die Datei mit keinem der verfügbaren Encodings lesen\")\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"Speichert oder aktualisiert die CSV-Datei mit deutschen Zahlenformat\"\"\"\n",
    "    try:\n",
    "        # Wenn Datei existiert, lade bestehende Daten\n",
    "        if os.path.exists(filename):\n",
    "            df_existing = pd.read_csv(filename, sep=';', decimal=',')\n",
    "            df_new = pd.DataFrame([data])\n",
    "            df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        else:\n",
    "            # Erste Daten\n",
    "            df_combined = pd.DataFrame([data])\n",
    "        \n",
    "        # Formatiere alle numerischen Spalten\n",
    "        for col in df_combined.columns:\n",
    "            if col != 'video_id':\n",
    "                # Konvertiere zu numerisch falls noch nicht geschehen\n",
    "                df_combined[col] = pd.to_numeric(df_combined[col], errors='ignore')\n",
    "                # Formatiere mit deutschem Format (Komma statt Punkt)\n",
    "                df_combined[col] = df_combined[col].apply(lambda x: f\"{x:.2f}\".replace('.', ','))\n",
    "        \n",
    "        # Speichere mit Semikolon als Trenner\n",
    "        df_combined.to_csv(filename, sep=';', index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(\"\\nDaten erfolgreich gespeichert\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Speichern: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def get_gender_age_analytics(analytics, video_id, start_date, end_date):\n",
    "    \"\"\"YouTube Analytics API Abfrage für Geschlechter- und Altersverteilung\"\"\"\n",
    "    try:\n",
    "        request = analytics.reports().query(\n",
    "            dimensions='ageGroup,gender',\n",
    "            ids=f'channel=={CHANNEL_ID}',\n",
    "            metrics='viewerPercentage',\n",
    "            filters=f'video=={video_id}',\n",
    "            startDate=start_date.strftime('%Y-%m-%d'),\n",
    "            endDate=end_date.strftime('%Y-%m-%d')\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Initialisiere Dictionary mit allen möglichen Altersgruppen\n",
    "        age_groups = ['13-17', '18-24', '25-34', '35-44', '45-54', '55-64', '65-']\n",
    "        data = {\n",
    "            'video_id': video_id\n",
    "        }\n",
    "        for age in age_groups:\n",
    "            data[f'audience_female_age{age}'] = 0.0  # Explizit als float\n",
    "            data[f'audience_male_age{age}'] = 0.0    # Explizit als float\n",
    "            \n",
    "        value_count = 0\n",
    "        total_percentage = 0\n",
    "            \n",
    "        if 'rows' in response:\n",
    "            print(f\"\\nGefundene Werte für Video {video_id}:\")\n",
    "            for row in response['rows']:\n",
    "                age_group = row[0]\n",
    "                gender = row[1]\n",
    "                percentage = float(row[2])  # Explizite Konvertierung zu float\n",
    "                \n",
    "                column_name = f'audience_{gender}_age{age_group}'\n",
    "                data[column_name] = percentage\n",
    "                print(f\"{column_name}: {percentage:.1f}%\")\n",
    "                \n",
    "                value_count += 1\n",
    "                total_percentage += percentage\n",
    "            \n",
    "            print(f\"\\nDatenqualität:\")\n",
    "            print(f\"Anzahl gefundener Werte: {value_count}\")\n",
    "            print(f\"Summe der Prozente: {total_percentage:.1f}%\")\n",
    "            \n",
    "            if value_count < 5:\n",
    "                print(\"Zu wenige demografische Daten vorhanden - Datensatz wird übersprungen\")\n",
    "                return None\n",
    "                \n",
    "            if total_percentage < 20:\n",
    "                print(\"Gesamtprozentsatz zu niedrig - Datensatz wird übersprungen\")\n",
    "                return None\n",
    "            \n",
    "            # Debug-Ausgabe der finalen Daten\n",
    "            print(\"\\nFinale Daten vor dem Speichern:\")\n",
    "            for key, value in data.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "                \n",
    "            return data\n",
    "        else:\n",
    "            print(f\"\\nKeine Daten für Video {video_id} gefunden\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        if 'quota' in str(e).lower():\n",
    "            print(f\"Quota-Limit erreicht bei Video {video_id}\")\n",
    "            raise e\n",
    "        print(f\"Fehler bei Video {video_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_videos(analytics, video_ids, start_date, end_date, start_index=0):\n",
    "    \"\"\"Verarbeitet Videos und zeigt Fortschritt\"\"\"\n",
    "    total_videos = len(video_ids)\n",
    "    start_time = time.time()\n",
    "    valid_data_count = 0\n",
    "    skipped_data_count = 0\n",
    "    \n",
    "    try:\n",
    "        for index, video_id in enumerate(video_ids[start_index:], start=start_index):\n",
    "            current_time = time.time()\n",
    "            videos_processed = index - start_index + 1\n",
    "            \n",
    "            if videos_processed > 0:\n",
    "                time_per_video = (current_time - start_time) / videos_processed\n",
    "                videos_remaining = total_videos - (index + 1)\n",
    "                estimated_time_remaining = timedelta(seconds=int(videos_remaining * time_per_video))\n",
    "                \n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Video {index + 1} von {total_videos}: {video_id}\")\n",
    "                print(f\"Durchschnittliche Zeit pro Video: {time_per_video:.1f} Sekunden\")\n",
    "                print(f\"Geschätzte verbleibende Zeit: {estimated_time_remaining}\")\n",
    "                print(f\"Bisher valide Datensätze: {valid_data_count}\")\n",
    "                print(f\"Übersprungene Datensätze: {skipped_data_count}\")\n",
    "            \n",
    "            data = get_gender_age_analytics(analytics, video_id, start_date, end_date)\n",
    "            if data:\n",
    "                if save_to_csv(data, OUTPUT_FILE):\n",
    "                    valid_data_count += 1\n",
    "                    print(f\"Daten in {OUTPUT_FILE} gespeichert\")\n",
    "            else:\n",
    "                skipped_data_count += 1\n",
    "            \n",
    "            # Speichere Checkpoint\n",
    "            with open(CHECKPOINT_FILE, 'w') as f:\n",
    "                json.dump({'last_processed_index': index}, f)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler aufgetreten: {str(e)}\")\n",
    "        print(f\"Letzter verarbeiteter Index: {index}\")\n",
    "        raise e\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Lade Video-IDs mit korrektem Encoding\n",
    "        print(\"Lade Video-IDs aus der bereinigten Datei...\")\n",
    "        video_df = read_csv_with_encoding(\"daten bereinigt dezimalzeichen fehlt in gender.csv\")\n",
    "        video_ids = video_df['video_id'].tolist()\n",
    "        print(f\"Geladen: {len(video_ids)} Videos\")\n",
    "        \n",
    "        # Lade letzten Fortschritt\n",
    "        start_index = -1\n",
    "        if os.path.exists(CHECKPOINT_FILE):\n",
    "            with open(CHECKPOINT_FILE, 'r') as f:\n",
    "                start_index = json.load(f)['last_processed_index']\n",
    "        \n",
    "        # Initialisiere API\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)\n",
    "        credentials = flow.run_local_server(port=0)\n",
    "        analytics = build('youtubeAnalytics', 'v2', credentials=credentials)\n",
    "        \n",
    "        # Setze Datumsgrenzen\n",
    "        start_date = datetime.strptime(START_DATE, '%d.%m.%Y')\n",
    "        end_date = datetime.now()\n",
    "        \n",
    "        print(f\"Starte Verarbeitung ab Index {start_index + 1}\")\n",
    "        process_videos(analytics, video_ids, start_date, end_date, start_index + 1)\n",
    "        \n",
    "        print(\"\\nVerarbeitung abgeschlossen!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler: {str(e)}\")\n",
    "        print(\"Sie können das Skript später neu starten - es wird am letzten Checkpoint fortgesetzt.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
